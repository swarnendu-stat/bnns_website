[{"id":0,"href":"/post/2025/04/18/bnns-binary-classification/","title":"BNNs in Binary Classification: A Bayesian Take on Clinical Prediction","section":"Posts","content":" üîç Can Bayesian Neural Nets Classify Better Than Your Favorite Models? # We often talk about uncertainty in models, but how often do we actually quantify it?\nEnter bnns, an R package that lets you fit Bayesian Neural Networks (BNNs) with the elegance of glm() and the insight of a statistician.\nIn this post, we‚Äôll walk through a real-world binary classification example: predicting respiratory status using clinical trial data. Then, we‚Äôll pit our BNN against the classic logistic regression and the ever-popular random forest.\nLet the models battle it out‚ÄîBayesian style.\nüß™ The Data: Clean, Clinical, and Real # We use the respiratory dataset from the HSAUR3 package, focusing on patients from month 4. The binary outcome is whether the patient\u0026rsquo;s status is \u0026ldquo;good\u0026rdquo;. Simple, interpretable, and grounded in reality‚Äîjust how statisticians like it.\nlibrary(bnns)\rlibrary(HSAUR3)\rWarning: package 'HSAUR3' was built under R version 4.4.3\rlibrary(rsample)\rlibrary(ggplot2)\rlibrary(randomForest)\rrandomForest 4.7-1.2\rType rfNews() to see new features/changes/bug fixes.\rAttaching package: 'randomForest'\rThe following object is masked from 'package:ggplot2':\rmargin\rset.seed(123)\rtrial_data \u0026lt;- HSAUR3::respiratory |\u0026gt;\rsubset(month == 4, select = - c(subject, month)) |\u0026gt;\rtransform(status = ifelse(status == \u0026quot;good\u0026quot;, 1, 0))\rtrial_data_split \u0026lt;- initial_split(trial_data, strata = status, prop = 0.8)\rtrial_data_train \u0026lt;- training(trial_data_split)\rtrial_data_test \u0026lt;- testing(trial_data_split)\rWe stratify by outcome during data splitting to ensure balance. BNNs appreciate thoughtful input.\nüß† Building the BNN: No Black Box Here # bnn_model \u0026lt;- bnns(\rstatus ~ .,\rdata = trial_data_train,\rL = 3,\rnodes = c(64, 4, 4),\ract_fn = rep(4, 3),\rout_act_fn = 2,\riter = 1e3,\rwarmup = 2e2,\rchains = 2,\rprior_weights = list(dist = \u0026quot;normal\u0026quot;, params = list(mean = 0, sd = 1)),\rprior_bias = list(dist = \u0026quot;normal\u0026quot;, params = list(mean = 0, sd = 10)),\rprior_sigma = list(dist = \u0026quot;half_normal\u0026quot;, params = list(mean = 0, sd = 1))\r)\rWith just a few lines, we‚Äôve defined a three-layer BNN with fully Bayesian uncertainty on weights, biases, and the residual scale. And no, you don‚Äôt need a PhD in Stan to do this‚Äîbnns handles the machinery under the hood.\nüîÆ Predictions with a Side of Uncertainty # Unlike point estimates from GLMs or RFs, BNNs give us full posterior distributions for predictions. We grab medians and 95% credible intervals‚Äîbecause what‚Äôs a prediction without a little humility?\npred \u0026lt;- predict(bnn_model, newdata = trial_data_test)\rpred_y \u0026lt;- apply(pred, MARGIN = 1, median, na.rm = TRUE)\rpred_quantiles \u0026lt;- apply(pred, MARGIN = 1, function(x)quantile(x, probs = c(0.025, 0.975), na.rm = TRUE), simplify = FALSE) |\u0026gt;\rdo.call(args = _, what = rbind)\rmeasure_bin(trial_data_test$status, pred)\rSetting levels: control = 0, case = 1\rSetting direction: controls \u0026lt; cases\r$conf_mat\rpred_label\robs 0 1\r0 1 10\r1 0 12\r$accuracy\r[1] 0.5652174\r$ROC\rCall:\rroc.default(response = obs, predictor = pred)\rData: pred in 11 controls (obs 0) \u0026lt; 12 cases (obs 1).\rArea under the curve: 0.7879\r$AUC\r[1] 0.7878788\rWe then evaluate performance using measure_bin(), our in-house accuracy function.\nüìä The Plot Thickens # We visualize predictions with credible intervals to see where the model hesitates‚Äîand that‚Äôs the beauty of Bayesian models. They don‚Äôt bluff.\nplot_data \u0026lt;- data.frame(\rActual = trial_data_test$status,\rPredicted = pred_y,\rLower = pred_quantiles[,1],\rUpper = pred_quantiles[,2]\r) |\u0026gt;\rtransform(width = Upper - Lower) |\u0026gt;\rdplyr::arrange(width)\rhead(plot_data, 5)\rActual Predicted Lower Upper width\r1 1 0.5185122 0.3228013 0.6419848 0.3191835\r2 0 0.5218692 0.3315534 0.6607562 0.3292028\r3 1 0.5217615 0.2956351 0.6571779 0.3615428\r4 1 0.5222414 0.2957324 0.6613963 0.3656639\r5 0 0.5159739 0.2793451 0.6482355 0.3688903\rBNN predictions with 95% credible intervals for test observations. While most intervals are reasonably narrow, wider intervals reflect situations where the model senses ambiguity ‚Äî not a bug, but a valuable signal.\nLet‚Äôs look at a sample of confident predictions with interpretable intervals.\nggplot(head(plot_data, 5), aes(x = Actual, y = Predicted)) +\rgeom_point(size = 2) +\rgeom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.05, color = \u0026quot;steelblue\u0026quot;) +\rlabs(title = \u0026quot;BNN Predictions for Patient Status\u0026quot;,\rsubtitle = \u0026quot;Error bars show 95% credible intervals\u0026quot;,\rx = \u0026quot;Test Set Patient Status\u0026quot;,\ry = \u0026quot;Predicted Patient Status\u0026quot;\r) +\rtheme_minimal()\rUnlike black-box predictions, BNNs offer a quantified view of uncertainty ‚Äî a key advantage in high-stakes decision-making.\nü•ä The Showdown: GLM vs RF vs BNN # glm_model \u0026lt;- glm(status ~ .,\rdata = trial_data_train)\rglm_pred \u0026lt;- predict(glm_model, trial_data_test)\rmeasure_bin(trial_data_test$status, glm_pred)\rSetting levels: control = 0, case = 1\rSetting direction: controls \u0026lt; cases\r$conf_mat\rpred_label\robs 0 1\r0 5 6\r1 2 10\r$accuracy\r[1] 0.6521739\r$ROC\rCall:\rroc.default(response = obs, predictor = pred)\rData: pred in 11 controls (obs 0) \u0026lt; 12 cases (obs 1).\rArea under the curve: 0.7121\r$AUC\r[1] 0.7121212\rrf_model \u0026lt;- randomForest(factor(status) ~ .,\rdata = trial_data_train)\rrf_pred \u0026lt;- predict(rf_model, trial_data_test, type = \u0026quot;prob\u0026quot;)[,2]\rmeasure_bin(trial_data_test$status, rf_pred)\rSetting levels: control = 0, case = 1\rSetting direction: controls \u0026lt; cases\r$conf_mat\rpred_label\robs 0 1\r0 8 3\r1 4 8\r$accuracy\r[1] 0.6956522\r$ROC\rCall:\rroc.default(response = obs, predictor = pred)\rData: pred in 11 controls (obs 0) \u0026lt; 12 cases (obs 1).\rArea under the curve: 0.7841\r$AUC\r[1] 0.7840909\rWe compare three models:\nGLM: The old reliable. Random Forest: The default go-to. BNN: The fresh Bayesian face in town. BNNs didn‚Äôt just hold their own‚Äîthey brought nuance. They‚Äôre not just saying ‚Äúyes‚Äù or ‚Äúno‚Äù, they‚Äôre saying ‚Äúprobably yes, and here‚Äôs how sure I am.‚Äù\nüí° Why Use bnns? # Uncertainty built-in: Every prediction comes with a measure of confidence. Flexible priors: Tailor regularization like a true Bayesian. Transparent: Uses rstan under the hood, no magic involved. Light syntax: As easy as glm(), but smarter. üöÄ Final Thoughts # In clinical settings, decisions aren‚Äôt just about accuracy‚Äîthey‚Äôre about confidence. That‚Äôs where BNNs shine. And with bnns, you don‚Äôt have to trade usability for rigor.\nNext time you\u0026rsquo;re modeling binary outcomes, ask yourself:\n\u0026gt; Do I just want a prediction, or do I want to understand the uncertainty behind it?\n"},{"id":1,"href":"/post/2025/01/17/bnns-for-tmle/","title":"Using bnns for TMLE","section":"Posts","content":" Introduction # This document demonstrates how to use the bnns package with TMLE for causal inference. TMLE combines machine learning-based flexible models with statistical principles to produce unbiased and efficient estimators of causal effects, such as the Average Treatment Effect (ATE). The example also highlights how the flexibility of Bayesian Neural Networks (BNNs) can improve TMLE results when handling complex data-generating mechanisms. This tutorial borrows from this tmle tutorial.\nSimulating Data # We simulate data where the treatment assignment and outcome are influenced by multiple covariates, and the true ATE is known.\nlibrary(bnns)\r# Set a random seed for reproducibility\rset.seed(123)\r# Simulate data\rn \u0026lt;- 1000 # Number of samples\rX1 \u0026lt;- rnorm(n)\rX2 \u0026lt;- rnorm(n)\rU \u0026lt;- rbinom(n, 1, 0.5) # Unmeasured confounder\r# Treatment mechanism (biased by U)\rps \u0026lt;- plogis(0.5 * X1 - 0.5 * X2 + 0.8 * U)\rA \u0026lt;- rbinom(n, 1, ps) # Treatment assignment\r# Outcome mechanism (non-linear and confounded by U)\rY_prob \u0026lt;- plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * A + 1.5 * U)\rY \u0026lt;- rbinom(n, 1, Y_prob)\r# Combine into a dataset\rsim_data \u0026lt;- data.frame(Y, A, X1, X2)\r# True ATE for comparison\rtrue_ate \u0026lt;- mean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 1 + 1.5 * U)) -\rmean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 0 + 1.5 * U))\rtrue_ate\r#\u0026gt; [1] 0.07431068\rApplying TMLE with bnns # Step 1: Install and Load Required Packages # Ensure that the bnns and tmle packages are installed.\n# Install ggplot2 and bnns from CRAN\rinstall.packages(\u0026quot;ggplot2\u0026quot;)\rinstall.packages(\u0026quot;bnns\u0026quot;)\rStep 2: TMLE Implementation # We use tmle to estimate the ATE. Both the treatment mechanism (g) and the outcome mechanism (Q) are modeled using Bayesian Neural Networks via the bnns package.\n### Step 1: Estimate Q\rQ \u0026lt;- bnns(Y ~ -1 + ., data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rQ_A \u0026lt;- predict(Q) # obtain predictions for everyone using the treatment they actually received\rsim_data_1 \u0026lt;- sim_data |\u0026gt; transform(A = 1) # data set where everyone received treatment\rQ_1 \u0026lt;- predict(Q, newdata = sim_data_1) # predict on that everyone-exposed data set\rsim_data_0 \u0026lt;- sim_data |\u0026gt; transform(A = 0) # data set where no one received treatment\rQ_0 \u0026lt;- predict(Q, newdata = sim_data_0)\rdat_tmle \u0026lt;- lapply(1:dim(Q_A)[2], function(i) data.frame(Y = sim_data$Y, A = sim_data$A, Q_A = Q_A[,i], Q_0 = Q_0[,i], Q_1 = Q_1[,i]))\r### Step 2: Estimate g and compute H(A,W)\rg \u0026lt;- bnns(A ~ -1 + . - Y, data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rg_w \u0026lt;- predict(g) # Pr(A=1|W)\rH_1 \u0026lt;- 1/g_w\rH_0 \u0026lt;- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)\rdat_tmle \u0026lt;- # add clever covariate data to dat_tmle\rlapply(1:dim(Q_A)[2], function(i) dat_tmle[[i]] |\u0026gt;\rcbind(\rH_1 = H_1[,i],\rH_0 = H_0[,i]) |\u0026gt;\rtransform(H_A = ifelse(A == 1, H_1, # if A is 1 (treated), assign H_1\rH_0)) # if A is 0 (not treated), assign H_0\r)\r### Step 3: Estimate fluctuation parameter\rtmle_ate_list \u0026lt;- lapply(1:dim(Q_A)[2], function(i){\rglm_fit \u0026lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle[[i]], family=binomial) # fixed intercept logistic regression\reps \u0026lt;- coef(glm_fit) # save the only coefficient, called epsilon in TMLE lit\r### Step 4: Update Q's\rQ_A_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_A) + eps*H_A)) # updated expected outcome given treatment actually received\rQ_1_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_1) + eps*H_1)) # updated expected outcome for everyone receiving treatment\rQ_0_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_0) + eps*H_0)) # updated expected outcome for everyone not receiving treatment\r### Step 5: Compute ATE\rtmle_ate \u0026lt;- mean(Q_1_update - Q_0_update) # mean diff in updated expected outcome estimates return(tmle_ate)\r})\rtmle_ate \u0026lt;- unlist(tmle_ate_list)\rmedian(tmle_ate)\r#\u0026gt; [1] 0.08951223\rComparing TMLE to Traditional Methods # To highlight the benefits of TMLE, compare it to other methods such as:\nIPTW (Inverse Probability of Treatment Weighting) Naive Comparison (Unadjusted Difference in Means) # IPTW\r# ps_model \u0026lt;- glm(A ~ .-Y, data = sim_data, family = binomial)\r# ps_pred \u0026lt;- predict(ps_model, type = \u0026quot;response\u0026quot;)\r# iptw_weights \u0026lt;- ifelse(sim_data$A == 1, 1 / ps_pred, 1 / (1 - ps_pred))\r# iptw_ate \u0026lt;- mean(iptw_weights * sim_data$Y * sim_data$A) -\r# mean(iptw_weights * sim_data$Y * (1 - sim_data$A))\rlibrary(tmle)\r#\u0026gt; Loading required package: glmnet\r#\u0026gt; Loading required package: Matrix\r#\u0026gt; Loaded glmnet 4.1-8\r#\u0026gt; Loading required package: SuperLearner\r#\u0026gt; Loading required package: nnls\r#\u0026gt; Loading required package: gam\r#\u0026gt; Loading required package: splines\r#\u0026gt; Loading required package: foreach\r#\u0026gt; Loaded gam 1.22-5\r#\u0026gt; Super Learner\r#\u0026gt; Version: 2.0-29\r#\u0026gt; Package created on 2024-02-06\r#\u0026gt; Welcome to the tmle package, version 2.0.1.1\r#\u0026gt; #\u0026gt; Use tmleNews() to see details on changes and bug fixes\rfreq_tmle \u0026lt;- tmle(\rY = sim_data$Y, A = sim_data$A, W = sim_data[, c(\u0026quot;X1\u0026quot;, \u0026quot;X2\u0026quot;)], Q.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;),\rg.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;)\r)\r#\u0026gt; Loading required namespace: ranger\r# Results\rfreq_tmle_ate \u0026lt;- freq_tmle$estimates$ATE$psi\r# Naive Comparison\rnaive_ate \u0026lt;- mean(sim_data$Y[sim_data$A == 1]) - mean(sim_data$Y[sim_data$A == 0])\r# Summary of Results\rresults \u0026lt;- data.frame(\rMethod = c(\u0026quot;True ATE\u0026quot;, \u0026quot;BNN_TMLE\u0026quot;, \u0026quot;Freq_TMLE\u0026quot;, \u0026quot;Naive\u0026quot;),\rEstimate = c(true_ate, median(tmle_ate), freq_tmle_ate, naive_ate),\rCI_low = c(true_ate, quantile(tmle_ate, probs = c(0.025)), freq_tmle$estimates$ATE$CI[1], naive_ate),\rCI_high = c(true_ate, quantile(tmle_ate, probs = c(0.975)), freq_tmle$estimates$ATE$CI[2], naive_ate)\r)\rresults\r#\u0026gt; Method Estimate CI_low CI_high\r#\u0026gt; 1 True ATE 0.07431068 0.07431068 0.07431068\r#\u0026gt; 2 BNN_TMLE 0.08951223 0.08370999 0.09612099\r#\u0026gt; 3 Freq_TMLE 0.09309979 0.05910546 0.12709413\r#\u0026gt; 4 Naive 0.12824940 0.12824940 0.12824940\rVisualizing Results # Plot the estimates from different methods to compare their performance.\nlibrary(ggplot2)\rggplot(results, aes(x = forcats::fct_reorder(Method, Estimate), y = Estimate, fill = Method)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, color = \u0026quot;black\u0026quot;) +\rgeom_hline(yintercept = true_ate, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) +\rgeom_errorbar(mapping = aes(ymin = CI_low, ymax = CI_high)) +\rlabs(\rtitle = \u0026quot;Comparison of ATE Estimates\u0026quot;,\ry = \u0026quot;ATE Estimate\u0026quot;,\rx = \u0026quot;Method\u0026quot;\r) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\rSummary # This tutorial demonstrates how to use the bnns package for TMLE to estimate the Average Treatment Effect (ATE). By leveraging the flexibility of Bayesian Neural Networks, TMLE can handle complex data structures and improve accuracy compared to traditional methods. Use this workflow for applications in clinical trials, epidemiology, and other domains requiring robust causal inference.\n"},{"id":2,"href":"/post/2025/01/17/power-of-base-r/","title":"Power of Base R: A Performance Comparison with dplyr","section":"Posts","content":" Introduction # This presentation explores the performance differences between base R and the dplyr package for various data manipulation tasks.\nWhile dplyr is renowned for its intuitive syntax and efficiency, base R functions can sometimes outperform it, particularly in large simulations. Understanding these differences can aid in making informed decisions when choosing data wrangling techniques.\nThe Iris Dataset # The iris dataset is a classic dataset in statistics and machine learning, often used for demonstrating data manipulation and analysis techniques.\nIt contains measurements of flower characteristics for three species of iris: setosa, versicolor, and virginica.\nKey Features:\n150 observations: 50 samples for each species.\n4 numeric attributes: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n1 categorical attribute: Species: The species of the iris flower (setosa, versicolor, virginica).\nThe Iris Dataset (continued) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r1 5.1 3.5 1.4 0.2 setosa\r2 4.9 3.0 1.4 0.2 setosa\r3 4.7 3.2 1.3 0.2 setosa\r51 7.0 3.2 4.7 1.4 versicolor\r52 6.4 3.2 4.5 1.5 versicolor\r53 6.9 3.1 4.9 1.5 versicolor\r101 6.3 3.3 6.0 2.5 virginica\r102 5.8 2.7 5.1 1.9 virginica\r103 7.1 3.0 5.9 2.1 virginica\rBenchmarking Data Manipulation Tasks # 1. Filter Rows # Filtering rows is a common operation in data analysis. This section benchmarks the efficiency of base R‚Äôs subsetting approaches against the dplyr::filter function. While dplyr provides clean syntax, base R‚Äôs performance is more efficient.\nmcb_filter \u0026lt;- microbenchmark(\rbase_0 = iris[iris$Species == \u0026quot;setosa\u0026quot;, ],\rbase_1 = subset(iris, Species == \u0026quot;setosa\u0026quot;),\rdplyr = filter(iris, Species == \u0026quot;setosa\u0026quot;),\rtimes = 1e3\r)\rmcb_filter\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 28.2 31.5 39.5817 35.8 47.10 219.6 1000\rbase_1 38.0 44.9 57.0533 54.7 65.00 1620.5 1000\rdplyr 347.7 384.7 420.2611 396.5 414.95 2603.2 1000\rautoplot(mcb_filter) + ggtitle(\u0026quot;Filter: Base R vs dplyr\u0026quot;)\ragg_filter \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_filter), mean)$time\rSo, base R code is on average 11 times faster than dplyr::filter. The absolute difference in 10k simulations is 3.806794 seconds.\n2. Select Columns # Selecting specific columns is fundamental to narrowing down datasets. The comparison here shows how base R indexing and the subset function stack up against dplyr::select in terms of speed.\nmcb_select \u0026lt;- microbenchmark(\rbase_0 = iris[, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Petal.Length\u0026quot;)],\rbase_1 = subset(iris, select = c(Sepal.Length, Petal.Length)),\rdplyr = select(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_select\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 5.8 6.7 10.7370 9.2 10.6 1388.9 1000\rbase_1 25.1 31.7 41.3153 40.9 47.6 1471.5 1000\rdplyr 406.2 436.2 467.0370 448.9 466.2 2545.5 1000\rautoplot(mcb_select) + ggtitle(\u0026quot;Select: Base R vs dplyr\u0026quot;)\ragg_select \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_select), mean)$time\rSo, base R code is on average 43 times faster than dplyr::select. The absolute difference in 10k simulations is 4.563 seconds.\n3. Add/Modify Columns # Adding or modifying columns is crucial for feature engineering. Base R provides multiple methods for this, which are benchmarked here against dplyr::mutate for their computational efficiency.\nmcb_mutate \u0026lt;- microbenchmark(\rbase_0 = {iris$Sepal_LbyW \u0026lt;- iris$Sepal.Length / iris$Sepal.Width},\rbase_1 = transform(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rdplyr = mutate(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rtimes = 1e3\r)\rmcb_mutate\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 3.2 4.40 6.4022 6.25 8.20 25.9 1000\rbase_1 27.3 38.70 50.2744 50.00 58.85 1450.9 1000\rdplyr 360.2 401.05 428.6732 415.65 431.45 1964.5 1000\rautoplot(mcb_mutate) + ggtitle(\u0026quot;Mutate: Base R vs dplyr\u0026quot;)\ragg_mutate \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_mutate), mean)$time\rSo, base R code is on average 67 times faster than dplyr::mutate. The absolute difference in 10k simulations is 4.22271 seconds.\n4. Summarise Data # Data summarization helps derive aggregate metrics. This section compares colMeans from base R with dplyr::summarise_if, illustrating their relative performance in summarizing numeric columns.\nmcb_summarise \u0026lt;- microbenchmark(\rbase_0 = with(iris, data.frame(Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width))),\rdplyr = summarise(iris, Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width)),\rtimes = 1e3\r)\rmcb_summarise\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 117.2 136.3 165.6190 166.40 179.10 3398.5 1000\rdplyr 589.1 651.8 697.3837 674.85 702.95 2235.8 1000\rautoplot(mcb_summarise) + ggtitle(\u0026quot;Summarise: Base R vs dplyr\u0026quot;)\ragg_summarise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_summarise), mean)$time\rSo, base R code is on average 4 times faster than dplyr::summarise. The absolute difference in 10k simulations is 5.317647 seconds.\n5. Grouped Summary # Aggregating data by groups is often required for advanced analytics. Base R‚Äôs aggregate function is tested here against dplyr\u0026rsquo;s group_by and summarise_all functions to highlight performance differences.\nmcb_grp \u0026lt;- microbenchmark(\rbase_0 = aggregate(. ~ Species, data = iris, FUN = mean, na.rm = TRUE),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rsummarise_all(mean, na.rm = TRUE) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 638.0 680.80 742.3042 720.70 744.55 2785.4 1000\rdplyr 1935.6 2014.55 2132.6932 2047.95 2110.20 5475.2 1000\rautoplot(mcb_grp) + ggtitle(\u0026quot;Grouped Summarise: Base R vs dplyr\u0026quot;)\ragg_grp \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp), mean)$time\rSo, base R code is on average 3 times faster than dplyr::grp. The absolute difference in 10k simulations is 13.90389 seconds.\n6. Sort Data # Sorting is essential for ordering data before visualization or further analysis. The benchmarks here compare the classic base R order approach to the elegant dplyr::arrange.\nmcb_sort \u0026lt;- microbenchmark(\rbase_0 = iris[with(iris, order(Sepal.Length, Petal.Length)), ],\rdplyr = arrange(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_sort\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 39.4 51.20 67.530 73.2 77.85 1301.3 1000\rdplyr 1296.7 1330.55 1432.962 1348.4 1396.05 39396.9 1000\rautoplot(mcb_sort) + ggtitle(\u0026quot;Sort: Base R vs dplyr\u0026quot;)\ragg_sort \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_sort), mean)$time\rSo, base R code is on average 21 times faster than dplyr::arrange. The absolute difference in 10k simulations is 13.654316 seconds.\n7. Join Data # Data joins are indispensable when working with relational datasets. This section demonstrates the efficiency of base R‚Äôs merge function compared to dplyr::left_join.\niris$id \u0026lt;- sample.int(nrow(iris), replace = FALSE)\riris2 \u0026lt;- iris[sample(nrow(iris)),]\rmcb_join \u0026lt;- microbenchmark(\rbase_0 = merge(iris, iris2, by = \u0026quot;id\u0026quot;, all.x = TRUE),\rdplyr = left_join(iris, iris2, by = \u0026quot;id\u0026quot;),\rtimes = 1e3\r)\rmcb_join\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 261.2 303.10 344.9787 337.05 359.15 2439.3 1000\rdplyr 612.9 671.35 718.9914 692.65 718.30 2661.7 1000\rautoplot(mcb_join) + ggtitle(\u0026quot;Join: Base R vs dplyr\u0026quot;)\ragg_join \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_join), mean)$time\rSo, base R code is on average 2 times faster than dplyr::left_join. The absolute difference in 10k simulations is 3.740127 seconds.\n8. Group and Apply Function 1 # Applying models or transformations to groups of data is a frequent task in statistical workflows. This benchmark showcases the performance of split and lapply in base R versus dplyr::group_map.\nmcb_grp_map \u0026lt;- microbenchmark(\rbase_0 = lapply(split(iris, iris$Species), function(x) lm(Sepal.Length ~ Petal.Length, data = x)),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_map(~ lm(Sepal.Length ~ Petal.Length, data = .x)),\rtimes = 1e3\r)\rmcb_grp_map\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 758.0 817.05 887.7093 843.75 870.8 2685.5 1000\rdplyr 2354.5 2422.50 2558.5587 2458.40 2534.1 5709.2 1000\rautoplot(mcb_grp_map) + ggtitle(\u0026quot;Group Map: Base R vs dplyr\u0026quot;)\ragg_grp_map \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_map), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_map. The absolute difference in 10k simulations is 16.708494 seconds.\n9. Group and Apply Function 2 # Complex operations on grouped data are common in analytical pipelines. This comparison highlights the performance of base R‚Äôs split and lapply with a row-binding step versus dplyr::group_modify.\nmcb_grp_mod \u0026lt;- microbenchmark(\rbase_0 = do.call(rbind, lapply(split(iris, iris$Species), function(x) data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = x))$coefficients))),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_modify(~ data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = .x))$coefficients)) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp_mod\rUnit: milliseconds\rexpr min lq mean median uq max neval\rbase_0 1.3785 1.47355 1.608932 1.50645 1.5597 40.7367 1000\rdplyr 4.3195 4.47495 4.716570 4.58045 4.7456 8.3596 1000\rautoplot(mcb_grp_mod) + ggtitle(\u0026quot;Group Modify: Base R vs dplyr\u0026quot;)\ragg_grp_mod \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_mod), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_modify. The absolute difference in 10k simulations is 31.076376 seconds.\n10. Rowwise # mcb_rowwise \u0026lt;- microbenchmark(base_0 = transform(iris, Sepal_sd = apply(cbind(Sepal.Length, Sepal.Width), 1, sd),\rPetal_sd = apply(cbind(Petal.Length, Petal.Width), 1, sd)),\rdplyr = iris |\u0026gt;\rdplyr::rowwise() |\u0026gt;\rdplyr::mutate(Sepal_sd = sd(c(Sepal.Length, Sepal.Width)),\rPetal_sd = sd(c(Petal.Length, Petal.Width))) |\u0026gt;\rdplyr::ungroup(),\rtimes = 1e3)\rautoplot(mcb_rowwise) + ggtitle(\u0026quot;Rowwise: Base R vs dplyr\u0026quot;)\ragg_rowwise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_rowwise), mean)$time\rSo, base R code is on average 3 times faster than dplyr::rowwise. The absolute difference in 10k simulations is 21.667426 seconds.\n11. Count Rows by Group # Counting the number of rows by group is a simple yet frequent operation. The comparison here emphasizes the efficiency of base R‚Äôs table function against dplyr::count.\nmcb_count \u0026lt;- microbenchmark(\rbase_0 = table(iris$Species),\rdplyr = iris |\u0026gt; count(Species),\rtimes = 1e3\r)\rmcb_count\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 14.9 21.05 32.5562 36.30 39.90 1374.4 1000\rdplyr 1298.7 1348.40 1433.3286 1373.65 1428.55 3183.0 1000\rautoplot(mcb_count) + ggtitle(\u0026quot;Count: Base R vs dplyr\u0026quot;)\ragg_count \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_count), mean)$time\rSo, base R code is on average 44 times faster than dplyr::count. The absolute difference in 10k simulations is 14.007724 seconds.\n12. Identify Distinct Rows # Identifying unique rows is crucial for deduplication. This benchmark compares base R‚Äôs unique function with dplyr::distinct, shedding light on performance differences for this operation.\nmcb_distinct \u0026lt;- microbenchmark(\rbase_0 = unique(rbind(iris, iris)),\rdplyr = distinct(rbind(iris, iris)),\rtimes = 1e3\r)\rmcb_distinct\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 564.2 598.0 656.7336 612.90 638.05 2461.0 1000\rdplyr 232.4 261.8 286.6282 274.35 289.75 2160.5 1000\rautoplot(mcb_distinct) + ggtitle(\u0026quot;Distinct: Base R vs dplyr\u0026quot;)\ragg_distinct \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_distinct), mean)$time\rSo, base R code is on average 2 times slower than dplyr::distinct. The absolute difference in 10k simulations is 3.701054 seconds.\nSummary # Given that these functions are very commonly used, it is fair to assume that these functions are used at least 5 times in a standard simulation code. If that simulation is repeated for 10k times, then the total gain we have by using base R is at least 10.7472875 minutes.\nConclusion # This analysis demonstrates\nthat while dplyr offers user-friendly functions and a consistent syntax, base R can often be faster for basic operations, especially with large datasets. The choice between base R and dplyr should consider both readability and computational efficiency, dependent on the scale and complexity of your data tasks.\nFor intensive data simulations, careful function choice can lead to significant performance gains, underscoring the importance of benchmarking and profiling in the R language. "},{"id":3,"href":"/about/","title":"About","section":"Welcome","content":" About Me # Welcome to my blog!\nI‚Äôm an experienced statistician and an avid R enthusiast. Over the years, R has become an integral part of my professional and personal toolkit. Whether it‚Äôs performing advanced statistical modeling, developing data visualizations, or exploring machine learning algorithms, I believe R empowers users to push the boundaries of data analysis.\nThis blog is my humble attempt to share what I‚Äôve learned and continue to learn. Here, you‚Äôll find tutorials, tips, and insights about:\nR programming best practices Statistical methods and modeling Data visualization techniques Machine learning in R Performance optimization and comparisons Tools like Shiny, ggplot2, and more I‚Äôll also share examples from real-world scenarios, such as clinical trial data analysis and simulation studies, where R has proven to be invaluable.\nIf you‚Äôre passionate about data science, programming, or statistics‚Äîor just starting your R journey‚Äîyou‚Äôre in the right place. Let‚Äôs grow and learn together, one R script at a time!\nThank you for stopping by, and happy coding!\n"}]