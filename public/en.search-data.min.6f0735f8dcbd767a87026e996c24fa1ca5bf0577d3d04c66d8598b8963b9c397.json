[{"id":0,"href":"/2025/01/17/bnns-for-tmle/","title":"Using bnns for TMLE","section":"Posts","content":"knitr::opts_chunk$set(\rcollapse = TRUE,\rcomment = \u0026quot;#\u0026gt;\u0026quot;\r)\rIntroduction # This document demonstrates how to use the bnns package with TMLE for causal inference. TMLE combines machine learning-based flexible models with statistical principles to produce unbiased and efficient estimators of causal effects, such as the Average Treatment Effect (ATE). The example also highlights how the flexibility of Bayesian Neural Networks (BNNs) can improve TMLE results when handling complex data-generating mechanisms. This tutorial borrows from this .\nSimulating Data # We simulate data where the treatment assignment and outcome are influenced by multiple covariates, and the true ATE is known.\nlibrary(bnns)\r# Set a random seed for reproducibility\rset.seed(123)\r# Simulate data\rn \u0026lt;- 1000 # Number of samples\rX1 \u0026lt;- rnorm(n)\rX2 \u0026lt;- rnorm(n)\rU \u0026lt;- rbinom(n, 1, 0.5) # Unmeasured confounder\r# Treatment mechanism (biased by U)\rps \u0026lt;- plogis(0.5 * X1 - 0.5 * X2 + 0.8 * U)\rA \u0026lt;- rbinom(n, 1, ps) # Treatment assignment\r# Outcome mechanism (non-linear and confounded by U)\rY_prob \u0026lt;- plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * A + 1.5 * U)\rY \u0026lt;- rbinom(n, 1, Y_prob)\r# Combine into a dataset\rsim_data \u0026lt;- data.frame(Y, A, X1, X2)\r# True ATE for comparison\rtrue_ate \u0026lt;- mean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 1 + 1.5 * U)) -\rmean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 0 + 1.5 * U))\rtrue_ate\rApplying TMLE with bnns # Step 1: Install and Load Required Packages # Ensure that the bnns and tmle packages are installed.\n# Install ggplot2 and bnns from CRAN\rinstall.packages(\u0026quot;ggplot2\u0026quot;)\rinstall.packages(\u0026quot;bnns\u0026quot;)\rStep 2: TMLE Implementation # We use tmle to estimate the ATE. Both the treatment mechanism (g) and the outcome mechanism (Q) are modeled using Bayesian Neural Networks via the bnns package.\n### Step 1: Estimate Q\rQ \u0026lt;- bnns(Y ~ -1 + ., data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rQ_A \u0026lt;- predict(Q) # obtain predictions for everyone using the treatment they actually received\rsim_data_1 \u0026lt;- sim_data |\u0026gt; transform(A = 1) # data set where everyone received treatment\rQ_1 \u0026lt;- predict(Q, newdata = sim_data_1) # predict on that everyone-exposed data set\rsim_data_0 \u0026lt;- sim_data |\u0026gt; transform(A = 0) # data set where no one received treatment\rQ_0 \u0026lt;- predict(Q, newdata = sim_data_0)\rdat_tmle \u0026lt;- lapply(1:dim(Q_A)[2], function(i) data.frame(Y = sim_data$Y, A = sim_data$A, Q_A = Q_A[,i], Q_0 = Q_0[,i], Q_1 = Q_1[,i]))\r### Step 2: Estimate g and compute H(A,W)\rg \u0026lt;- bnns(A ~ -1 + . - Y, data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rg_w \u0026lt;- predict(g) # Pr(A=1|W)\rH_1 \u0026lt;- 1/g_w\rH_0 \u0026lt;- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)\rdat_tmle \u0026lt;- # add clever covariate data to dat_tmle\rlapply(1:dim(Q_A)[2], function(i) dat_tmle[[i]] |\u0026gt;\rcbind(\rH_1 = H_1[,i],\rH_0 = H_0[,i]) |\u0026gt;\rtransform(H_A = ifelse(A == 1, H_1, # if A is 1 (treated), assign H_1\rH_0)) # if A is 0 (not treated), assign H_0\r)\r### Step 3: Estimate fluctuation parameter\rtmle_ate_list \u0026lt;- lapply(1:dim(Q_A)[2], function(i){\rglm_fit \u0026lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle[[i]], family=binomial) # fixed intercept logistic regression\reps \u0026lt;- coef(glm_fit) # save the only coefficient, called epsilon in TMLE lit\r### Step 4: Update Q's\rQ_A_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_A) + eps*H_A)) # updated expected outcome given treatment actually received\rQ_1_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_1) + eps*H_1)) # updated expected outcome for everyone receiving treatment\rQ_0_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_0) + eps*H_0)) # updated expected outcome for everyone not receiving treatment\r### Step 5: Compute ATE\rtmle_ate \u0026lt;- mean(Q_1_update - Q_0_update) # mean diff in updated expected outcome estimates return(tmle_ate)\r})\rtmle_ate \u0026lt;- unlist(tmle_ate_list)\rmedian(tmle_ate)\rComparing TMLE to Traditional Methods # To highlight the benefits of TMLE, compare it to other methods such as:\nIPTW (Inverse Probability of Treatment Weighting) Naive Comparison (Unadjusted Difference in Means) # IPTW\r# ps_model \u0026lt;- glm(A ~ .-Y, data = sim_data, family = binomial)\r# ps_pred \u0026lt;- predict(ps_model, type = \u0026quot;response\u0026quot;)\r# iptw_weights \u0026lt;- ifelse(sim_data$A == 1, 1 / ps_pred, 1 / (1 - ps_pred))\r# iptw_ate \u0026lt;- mean(iptw_weights * sim_data$Y * sim_data$A) -\r# mean(iptw_weights * sim_data$Y * (1 - sim_data$A))\rlibrary(tmle)\rfreq_tmle \u0026lt;- tmle(\rY = sim_data$Y, A = sim_data$A, W = sim_data[, c(\u0026quot;X1\u0026quot;, \u0026quot;X2\u0026quot;)], Q.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;),\rg.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;)\r)\r# Results\rfreq_tmle_ate \u0026lt;- freq_tmle$estimates$ATE$psi\r# Naive Comparison\rnaive_ate \u0026lt;- mean(sim_data$Y[sim_data$A == 1]) - mean(sim_data$Y[sim_data$A == 0])\r# Summary of Results\rresults \u0026lt;- data.frame(\rMethod = c(\u0026quot;True ATE\u0026quot;, \u0026quot;BNN_TMLE\u0026quot;, \u0026quot;Freq_TMLE\u0026quot;, \u0026quot;Naive\u0026quot;),\rEstimate = c(true_ate, median(tmle_ate), freq_tmle_ate, naive_ate),\rCI_low = c(true_ate, quantile(tmle_ate, probs = c(0.025)), freq_tmle$estimates$ATE$CI[1], naive_ate),\rCI_high = c(true_ate, quantile(tmle_ate, probs = c(0.975)), freq_tmle$estimates$ATE$CI[2], naive_ate)\r)\rresults\rVisualizing Results # Plot the estimates from different methods to compare their performance.\nlibrary(ggplot2)\rggplot(results, aes(x = forcats::fct_reorder(Method, Estimate), y = Estimate, fill = Method)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, color = \u0026quot;black\u0026quot;) +\rgeom_hline(yintercept = true_ate, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) +\rgeom_errorbar(mapping = aes(ymin = CI_low, ymax = CI_high)) +\rlabs(\rtitle = \u0026quot;Comparison of ATE Estimates\u0026quot;,\ry = \u0026quot;ATE Estimate\u0026quot;,\rx = \u0026quot;Method\u0026quot;\r) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\rSummary # This tutorial demonstrates how to use the bnns package for TMLE to estimate the Average Treatment Effect (ATE). By leveraging the flexibility of Bayesian Neural Networks, TMLE can handle complex data structures and improve accuracy compared to traditional methods. Use this workflow for applications in clinical trials, epidemiology, and other domains requiring robust causal inference.\n"},{"id":1,"href":"/2025/01/17/power-of-base-r-a-performance-comparison-with-dplyr/","title":"Power of Base R: A Performance Comparison with dplyr","section":"Posts","content":" Introduction # This presentation explores the performance differences between base R and the dplyr package for various data manipulation tasks.\nWhile dplyr is renowned for its intuitive syntax and efficiency, base R functions can sometimes outperform it, particularly in large simulations. Understanding these differences can aid in making informed decisions when choosing data wrangling techniques.\nThe Iris Dataset # The iris dataset is a classic dataset in statistics and machine learning, often used for demonstrating data manipulation and analysis techniques.\nIt contains measurements of flower characteristics for three species of iris: setosa, versicolor, and virginica.\nKey Features:\n150 observations: 50 samples for each species.\n4 numeric attributes: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n1 categorical attribute: Species: The species of the iris flower (setosa, versicolor, virginica).\nThe Iris Dataset (continued) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r1 5.1 3.5 1.4 0.2 setosa\r2 4.9 3.0 1.4 0.2 setosa\r3 4.7 3.2 1.3 0.2 setosa\r51 7.0 3.2 4.7 1.4 versicolor\r52 6.4 3.2 4.5 1.5 versicolor\r53 6.9 3.1 4.9 1.5 versicolor\r101 6.3 3.3 6.0 2.5 virginica\r102 5.8 2.7 5.1 1.9 virginica\r103 7.1 3.0 5.9 2.1 virginica\rBenchmarking Data Manipulation Tasks # 1. Filter Rows # Filtering rows is a common operation in data analysis. This section benchmarks the efficiency of base R’s subsetting approaches against the dplyr::filter function. While dplyr provides clean syntax, base R’s performance is more efficient.\nmcb_filter \u0026lt;- microbenchmark(\rbase_0 = iris[iris$Species == \u0026quot;setosa\u0026quot;, ],\rbase_1 = subset(iris, Species == \u0026quot;setosa\u0026quot;),\rdplyr = filter(iris, Species == \u0026quot;setosa\u0026quot;),\rtimes = 1e3\r)\rmcb_filter\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 28.7 34.7 46.6746 40.60 52.0 1826.2 1000\rbase_1 39.0 49.4 65.7417 60.20 72.1 2014.9 1000\rdplyr 350.5 415.5 470.6557 433.35 460.0 3044.6 1000\rautoplot(mcb_filter) + ggtitle(\u0026quot;Filter: Base R vs dplyr\u0026quot;)\ragg_filter \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_filter), mean)$time\rSo, base R code is on average r max(round(max(agg_filter)/agg_filter)) times faster than dplyr::filter. The absolute difference in 10k simulations is 4.239811 seconds.\n2. Select Columns # Selecting specific columns is fundamental to narrowing down datasets. The comparison here shows how base R indexing and the subset function stack up against dplyr::select in terms of speed.\nmcb_select \u0026lt;- microbenchmark(\rbase_0 = iris[, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Petal.Length\u0026quot;)],\rbase_1 = subset(iris, select = c(Sepal.Length, Petal.Length)),\rdplyr = select(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_select\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 5.9 7.30 9.9381 10.20 11.40 133.4 1000\rbase_1 26.1 34.20 45.6359 44.80 52.15 1535.4 1000\rdplyr 407.5 464.85 507.9932 485.55 507.95 2373.0 1000\rautoplot(mcb_select) + ggtitle(\u0026quot;Select: Base R vs dplyr\u0026quot;)\ragg_select \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_select), mean)$time\rSo, base R code is on average 51 times faster than dplyr::select. The absolute difference in 10k simulations is 4.980551 seconds.\n3. Add/Modify Columns # Adding or modifying columns is crucial for feature engineering. Base R provides multiple methods for this, which are benchmarked here against dplyr::mutate for their computational efficiency.\nmcb_mutate \u0026lt;- microbenchmark(\rbase_0 = {iris$Sepal_LbyW \u0026lt;- iris$Sepal.Length / iris$Sepal.Width},\rbase_1 = transform(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rdplyr = mutate(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rtimes = 1e3\r)\rmcb_mutate\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 3.3 4.50 6.9702 7.3 8.70 116.1 1000\rbase_1 27.5 40.75 55.7073 53.6 62.10 1484.9 1000\rdplyr 358.1 417.40 455.0686 435.1 462.25 1972.6 1000\rautoplot(mcb_mutate) + ggtitle(\u0026quot;Mutate: Base R vs dplyr\u0026quot;)\ragg_mutate \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_mutate), mean)$time\rSo, base R code is on average 65 times faster than dplyr::mutate. The absolute difference in 10k simulations is 4.480984 seconds.\n4. Summarise Data # Data summarization helps derive aggregate metrics. This section compares colMeans from base R with dplyr::summarise_if, illustrating their relative performance in summarizing numeric columns.\nmcb_summarise \u0026lt;- microbenchmark(\rbase_0 = with(iris, data.frame(Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width))),\rdplyr = summarise(iris, Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width)),\rtimes = 1e3\r)\rmcb_summarise\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 125.4 147.4 179.0355 179.05 194.30 1620.5 1000\rdplyr 622.1 698.0 766.5408 725.40 754.35 4278.4 1000\rautoplot(mcb_summarise) + ggtitle(\u0026quot;Summarise: Base R vs dplyr\u0026quot;)\ragg_summarise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_summarise), mean)$time\rSo, base R code is on average 4 times faster than dplyr::summarise. The absolute difference in 10k simulations is 5.875053 seconds.\n5. Grouped Summary # Aggregating data by groups is often required for advanced analytics. Base R’s aggregate function is tested here against dplyr\u0026rsquo;s group_by and summarise_all functions to highlight performance differences.\nmcb_grp \u0026lt;- microbenchmark(\rbase_0 = aggregate(. ~ Species, data = iris, FUN = mean, na.rm = TRUE),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rsummarise_all(mean, na.rm = TRUE) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 636.1 745.15 826.6784 791.70 841.90 2784.3 1000\rdplyr 1969.4 2192.15 2366.5016 2282.95 2398.75 7188.3 1000\rautoplot(mcb_grp) + ggtitle(\u0026quot;Grouped Summarise: Base R vs dplyr\u0026quot;)\ragg_grp \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp), mean)$time\rSo, base R code is on average 3 times faster than dplyr::grp. The absolute difference in 10k simulations is 15.398232 seconds.\n6. Sort Data # Sorting is essential for ordering data before visualization or further analysis. The benchmarks here compare the classic base R order approach to the elegant dplyr::arrange.\nmcb_sort \u0026lt;- microbenchmark(\rbase_0 = iris[with(iris, order(Sepal.Length, Petal.Length)), ],\rdplyr = arrange(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_sort\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 39.7 53.90 71.4058 74.35 86.8 270.0 1000\rdplyr 1280.6 1420.85 1558.5407 1472.50 1523.8 42303.6 1000\rautoplot(mcb_sort) + ggtitle(\u0026quot;Sort: Base R vs dplyr\u0026quot;)\ragg_sort \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_sort), mean)$time\rSo, base R code is on average 22 times faster than dplyr::arrange. The absolute difference in 10k simulations is 14.871349 seconds.\n7. Join Data # Data joins are indispensable when working with relational datasets. This section demonstrates the efficiency of base R’s merge function compared to dplyr::left_join.\niris$id \u0026lt;- sample.int(nrow(iris), replace = FALSE)\riris2 \u0026lt;- iris[sample(nrow(iris)),]\rmcb_join \u0026lt;- microbenchmark(\rbase_0 = merge(iris, iris2, by = \u0026quot;id\u0026quot;, all.x = TRUE),\rdplyr = left_join(iris, iris2, by = \u0026quot;id\u0026quot;),\rtimes = 1e3\r)\rmcb_join\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 271.4 318.05 363.3037 352.3 372.75 2509.8 1000\rdplyr 619.4 698.20 749.3533 721.6 747.70 2761.8 1000\rautoplot(mcb_join) + ggtitle(\u0026quot;Join: Base R vs dplyr\u0026quot;)\ragg_join \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_join), mean)$time\rSo, base R code is on average 2 times faster than dplyr::left_join. The absolute difference in 10k simulations is 3.860496 seconds.\n8. Group and Apply Function 1 # Applying models or transformations to groups of data is a frequent task in statistical workflows. This benchmark showcases the performance of split and lapply in base R versus dplyr::group_map.\nmcb_grp_map \u0026lt;- microbenchmark(\rbase_0 = lapply(split(iris, iris$Species), function(x) lm(Sepal.Length ~ Petal.Length, data = x)),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_map(~ lm(Sepal.Length ~ Petal.Length, data = .x)),\rtimes = 1e3\r)\rmcb_grp_map\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 752.1 827.15 920.375 856.65 919.0 4556.8 1000\rdplyr 2336.8 2407.30 2640.159 2494.95 2709.1 6824.2 1000\rautoplot(mcb_grp_map) + ggtitle(\u0026quot;Group Map: Base R vs dplyr\u0026quot;)\ragg_grp_map \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_map), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_map. The absolute difference in 10k simulations is 17.197841 seconds.\n9. Group and Apply Function 2 # Complex operations on grouped data are common in analytical pipelines. This comparison highlights the performance of base R’s split and lapply with a row-binding step versus dplyr::group_modify.\nmcb_grp_mod \u0026lt;- microbenchmark(\rbase_0 = do.call(rbind, lapply(split(iris, iris$Species), function(x) data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = x))$coefficients))),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_modify(~ data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = .x))$coefficients)) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp_mod\rUnit: milliseconds\rexpr min lq mean median uq max neval\rbase_0 1.3756 1.46055 1.583922 1.49600 1.59545 4.7176 1000\rdplyr 4.3042 4.43040 4.798130 4.56795 4.86915 42.2420 1000\rautoplot(mcb_grp_mod) + ggtitle(\u0026quot;Group Modify: Base R vs dplyr\u0026quot;)\ragg_grp_mod \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_mod), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_modify. The absolute difference in 10k simulations is 32.142084 seconds.\n10. Rowwise # mcb_rowwise \u0026lt;- microbenchmark(base_0 = transform(iris, Sepal_sd = apply(cbind(Sepal.Length, Sepal.Width), 1, sd),\rPetal_sd = apply(cbind(Petal.Length, Petal.Width), 1, sd)),\rdplyr = iris |\u0026gt;\rdplyr::rowwise() |\u0026gt;\rdplyr::mutate(Sepal_sd = sd(c(Sepal.Length, Sepal.Width)),\rPetal_sd = sd(c(Petal.Length, Petal.Width))) |\u0026gt;\rdplyr::ungroup(),\rtimes = 1e3)\rautoplot(mcb_rowwise) + ggtitle(\u0026quot;Rowwise: Base R vs dplyr\u0026quot;)\ragg_rowwise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_rowwise), mean)$time\rSo, base R code is on average 3 times faster than dplyr::rowwise. The absolute difference in 10k simulations is 22.57265 seconds.\n11. Count Rows by Group # Counting the number of rows by group is a simple yet frequent operation. The comparison here emphasizes the efficiency of base R’s table function against dplyr::count.\nmcb_count \u0026lt;- microbenchmark(\rbase_0 = table(iris$Species),\rdplyr = iris |\u0026gt; count(Species),\rtimes = 1e3\r)\rmcb_count\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 15.2 21.05 32.9517 36.00 44.00 77.2 1000\rdplyr 1297.6 1411.90 1537.6797 1473.15 1540.95 4737.7 1000\rautoplot(mcb_count) + ggtitle(\u0026quot;Count: Base R vs dplyr\u0026quot;)\ragg_count \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_count), mean)$time\rSo, base R code is on average 47 times faster than dplyr::count. The absolute difference in 10k simulations is 15.04728 seconds.\n12. Identify Distinct Rows # Identifying unique rows is crucial for deduplication. This benchmark compares base R’s unique function with dplyr::distinct, shedding light on performance differences for this operation.\nmcb_distinct \u0026lt;- microbenchmark(\rbase_0 = unique(rbind(iris, iris)),\rdplyr = distinct(rbind(iris, iris)),\rtimes = 1e3\r)\rmcb_distinct\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 577.8 639.25 703.4088 658.8 682.8 3716.1 1000\rdplyr 243.3 279.30 315.5977 294.5 311.9 2243.3 1000\rautoplot(mcb_distinct) + ggtitle(\u0026quot;Distinct: Base R vs dplyr\u0026quot;)\ragg_distinct \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_distinct), mean)$time\rSo, base R code is on average 2 times slower than dplyr::distinct. The absolute difference in 10k simulations is 3.878111 seconds.\nSummary # Given that these functions are very commonly used, it is fair to assume that these functions are used at least 5 times in a standard simulation code. If that simulation is repeated for 10k times, then the total gain we have by using base R is at least 11.3990183 minutes.\nConclusion # This analysis demonstrates\nthat while dplyr offers user-friendly functions and a consistent syntax, base R can often be faster for basic operations, especially with large datasets. The choice between base R and dplyr should consider both readability and computational efficiency, dependent on the scale and complexity of your data tasks.\nFor intensive data simulations, careful function choice can lead to significant performance gains, underscoring the importance of benchmarking and profiling in the R language. "},{"id":2,"href":"/about/","title":"About","section":"Statistically Speaking: An R Journey","content":" About Me # Welcome to my blog!\nI’m an experienced statistician and an avid R enthusiast. Over the years, R has become an integral part of my professional and personal toolkit. Whether it’s performing advanced statistical modeling, developing data visualizations, or exploring machine learning algorithms, I believe R empowers users to push the boundaries of data analysis.\nThis blog is my humble attempt to share what I’ve learned and continue to learn. Here, you’ll find tutorials, tips, and insights about:\nR programming best practices Statistical methods and modeling Data visualization techniques Machine learning in R Performance optimization and comparisons Tools like Shiny, ggplot2, and more I’ll also share examples from real-world scenarios, such as clinical trial data analysis and simulation studies, where R has proven to be invaluable.\nIf you’re passionate about data science, programming, or statistics—or just starting your R journey—you’re in the right place. Let’s grow and learn together, one R script at a time!\nThank you for stopping by, and happy coding!\n"}]