[{"id":0,"href":"/2025/01/17/bnns-for-tmle/","title":"Using bnns for TMLE","section":"Posts","content":" Introduction # This document demonstrates how to use the bnns package with TMLE for causal inference. TMLE combines machine learning-based flexible models with statistical principles to produce unbiased and efficient estimators of causal effects, such as the Average Treatment Effect (ATE). The example also highlights how the flexibility of Bayesian Neural Networks (BNNs) can improve TMLE results when handling complex data-generating mechanisms. This tutorial borrows from this tmle tutorial.\nSimulating Data # We simulate data where the treatment assignment and outcome are influenced by multiple covariates, and the true ATE is known.\nlibrary(bnns)\r# Set a random seed for reproducibility\rset.seed(123)\r# Simulate data\rn \u0026lt;- 1000 # Number of samples\rX1 \u0026lt;- rnorm(n)\rX2 \u0026lt;- rnorm(n)\rU \u0026lt;- rbinom(n, 1, 0.5) # Unmeasured confounder\r# Treatment mechanism (biased by U)\rps \u0026lt;- plogis(0.5 * X1 - 0.5 * X2 + 0.8 * U)\rA \u0026lt;- rbinom(n, 1, ps) # Treatment assignment\r# Outcome mechanism (non-linear and confounded by U)\rY_prob \u0026lt;- plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * A + 1.5 * U)\rY \u0026lt;- rbinom(n, 1, Y_prob)\r# Combine into a dataset\rsim_data \u0026lt;- data.frame(Y, A, X1, X2)\r# True ATE for comparison\rtrue_ate \u0026lt;- mean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 1 + 1.5 * U)) -\rmean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 0 + 1.5 * U))\rtrue_ate\r#\u0026gt; [1] 0.07431068\rApplying TMLE with bnns # Step 1: Install and Load Required Packages # Ensure that the bnns and tmle packages are installed.\n# Install ggplot2 and bnns from CRAN\rinstall.packages(\u0026quot;ggplot2\u0026quot;)\rinstall.packages(\u0026quot;bnns\u0026quot;)\rStep 2: TMLE Implementation # We use tmle to estimate the ATE. Both the treatment mechanism (g) and the outcome mechanism (Q) are modeled using Bayesian Neural Networks via the bnns package.\n### Step 1: Estimate Q\rQ \u0026lt;- bnns(Y ~ -1 + ., data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rQ_A \u0026lt;- predict(Q) # obtain predictions for everyone using the treatment they actually received\rsim_data_1 \u0026lt;- sim_data |\u0026gt; transform(A = 1) # data set where everyone received treatment\rQ_1 \u0026lt;- predict(Q, newdata = sim_data_1) # predict on that everyone-exposed data set\rsim_data_0 \u0026lt;- sim_data |\u0026gt; transform(A = 0) # data set where no one received treatment\rQ_0 \u0026lt;- predict(Q, newdata = sim_data_0)\rdat_tmle \u0026lt;- lapply(1:dim(Q_A)[2], function(i) data.frame(Y = sim_data$Y, A = sim_data$A, Q_A = Q_A[,i], Q_0 = Q_0[,i], Q_1 = Q_1[,i]))\r### Step 2: Estimate g and compute H(A,W)\rg \u0026lt;- bnns(A ~ -1 + . - Y, data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rg_w \u0026lt;- predict(g) # Pr(A=1|W)\rH_1 \u0026lt;- 1/g_w\rH_0 \u0026lt;- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)\rdat_tmle \u0026lt;- # add clever covariate data to dat_tmle\rlapply(1:dim(Q_A)[2], function(i) dat_tmle[[i]] |\u0026gt;\rcbind(\rH_1 = H_1[,i],\rH_0 = H_0[,i]) |\u0026gt;\rtransform(H_A = ifelse(A == 1, H_1, # if A is 1 (treated), assign H_1\rH_0)) # if A is 0 (not treated), assign H_0\r)\r### Step 3: Estimate fluctuation parameter\rtmle_ate_list \u0026lt;- lapply(1:dim(Q_A)[2], function(i){\rglm_fit \u0026lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle[[i]], family=binomial) # fixed intercept logistic regression\reps \u0026lt;- coef(glm_fit) # save the only coefficient, called epsilon in TMLE lit\r### Step 4: Update Q's\rQ_A_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_A) + eps*H_A)) # updated expected outcome given treatment actually received\rQ_1_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_1) + eps*H_1)) # updated expected outcome for everyone receiving treatment\rQ_0_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_0) + eps*H_0)) # updated expected outcome for everyone not receiving treatment\r### Step 5: Compute ATE\rtmle_ate \u0026lt;- mean(Q_1_update - Q_0_update) # mean diff in updated expected outcome estimates return(tmle_ate)\r})\rtmle_ate \u0026lt;- unlist(tmle_ate_list)\rmedian(tmle_ate)\r#\u0026gt; [1] 0.08945627\rComparing TMLE to Traditional Methods # To highlight the benefits of TMLE, compare it to other methods such as:\nIPTW (Inverse Probability of Treatment Weighting) Naive Comparison (Unadjusted Difference in Means) # IPTW\r# ps_model \u0026lt;- glm(A ~ .-Y, data = sim_data, family = binomial)\r# ps_pred \u0026lt;- predict(ps_model, type = \u0026quot;response\u0026quot;)\r# iptw_weights \u0026lt;- ifelse(sim_data$A == 1, 1 / ps_pred, 1 / (1 - ps_pred))\r# iptw_ate \u0026lt;- mean(iptw_weights * sim_data$Y * sim_data$A) -\r# mean(iptw_weights * sim_data$Y * (1 - sim_data$A))\rlibrary(tmle)\r#\u0026gt; Loading required package: glmnet\r#\u0026gt; Loading required package: Matrix\r#\u0026gt; Loaded glmnet 4.1-8\r#\u0026gt; Loading required package: SuperLearner\r#\u0026gt; Loading required package: nnls\r#\u0026gt; Loading required package: gam\r#\u0026gt; Loading required package: splines\r#\u0026gt; Loading required package: foreach\r#\u0026gt; Loaded gam 1.22-5\r#\u0026gt; Super Learner\r#\u0026gt; Version: 2.0-29\r#\u0026gt; Package created on 2024-02-06\r#\u0026gt; Welcome to the tmle package, version 2.0.1.1\r#\u0026gt; #\u0026gt; Use tmleNews() to see details on changes and bug fixes\rfreq_tmle \u0026lt;- tmle(\rY = sim_data$Y, A = sim_data$A, W = sim_data[, c(\u0026quot;X1\u0026quot;, \u0026quot;X2\u0026quot;)], Q.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;),\rg.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;)\r)\r#\u0026gt; Loading required namespace: ranger\r# Results\rfreq_tmle_ate \u0026lt;- freq_tmle$estimates$ATE$psi\r# Naive Comparison\rnaive_ate \u0026lt;- mean(sim_data$Y[sim_data$A == 1]) - mean(sim_data$Y[sim_data$A == 0])\r# Summary of Results\rresults \u0026lt;- data.frame(\rMethod = c(\u0026quot;True ATE\u0026quot;, \u0026quot;BNN_TMLE\u0026quot;, \u0026quot;Freq_TMLE\u0026quot;, \u0026quot;Naive\u0026quot;),\rEstimate = c(true_ate, median(tmle_ate), freq_tmle_ate, naive_ate),\rCI_low = c(true_ate, quantile(tmle_ate, probs = c(0.025)), freq_tmle$estimates$ATE$CI[1], naive_ate),\rCI_high = c(true_ate, quantile(tmle_ate, probs = c(0.975)), freq_tmle$estimates$ATE$CI[2], naive_ate)\r)\rresults\r#\u0026gt; Method Estimate CI_low CI_high\r#\u0026gt; 1 True ATE 0.07431068 0.07431068 0.07431068\r#\u0026gt; 2 BNN_TMLE 0.08945627 0.08373730 0.09559821\r#\u0026gt; 3 Freq_TMLE 0.09309979 0.05910546 0.12709413\r#\u0026gt; 4 Naive 0.12824940 0.12824940 0.12824940\rVisualizing Results # Plot the estimates from different methods to compare their performance.\nlibrary(ggplot2)\rggplot(results, aes(x = forcats::fct_reorder(Method, Estimate), y = Estimate, fill = Method)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, color = \u0026quot;black\u0026quot;) +\rgeom_hline(yintercept = true_ate, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) +\rgeom_errorbar(mapping = aes(ymin = CI_low, ymax = CI_high)) +\rlabs(\rtitle = \u0026quot;Comparison of ATE Estimates\u0026quot;,\ry = \u0026quot;ATE Estimate\u0026quot;,\rx = \u0026quot;Method\u0026quot;\r) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\rSummary # This tutorial demonstrates how to use the bnns package for TMLE to estimate the Average Treatment Effect (ATE). By leveraging the flexibility of Bayesian Neural Networks, TMLE can handle complex data structures and improve accuracy compared to traditional methods. Use this workflow for applications in clinical trials, epidemiology, and other domains requiring robust causal inference.\n"},{"id":1,"href":"/2025/01/17/power-of-base-r/","title":"Power of Base R: A Performance Comparison with dplyr","section":"Posts","content":" Introduction # This presentation explores the performance differences between base R and the dplyr package for various data manipulation tasks.\nWhile dplyr is renowned for its intuitive syntax and efficiency, base R functions can sometimes outperform it, particularly in large simulations. Understanding these differences can aid in making informed decisions when choosing data wrangling techniques.\nThe Iris Dataset # The iris dataset is a classic dataset in statistics and machine learning, often used for demonstrating data manipulation and analysis techniques.\nIt contains measurements of flower characteristics for three species of iris: setosa, versicolor, and virginica.\nKey Features:\n150 observations: 50 samples for each species.\n4 numeric attributes: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n1 categorical attribute: Species: The species of the iris flower (setosa, versicolor, virginica).\nThe Iris Dataset (continued) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r1 5.1 3.5 1.4 0.2 setosa\r2 4.9 3.0 1.4 0.2 setosa\r3 4.7 3.2 1.3 0.2 setosa\r51 7.0 3.2 4.7 1.4 versicolor\r52 6.4 3.2 4.5 1.5 versicolor\r53 6.9 3.1 4.9 1.5 versicolor\r101 6.3 3.3 6.0 2.5 virginica\r102 5.8 2.7 5.1 1.9 virginica\r103 7.1 3.0 5.9 2.1 virginica\rBenchmarking Data Manipulation Tasks # 1. Filter Rows # Filtering rows is a common operation in data analysis. This section benchmarks the efficiency of base R’s subsetting approaches against the dplyr::filter function. While dplyr provides clean syntax, base R’s performance is more efficient.\nmcb_filter \u0026lt;- microbenchmark(\rbase_0 = iris[iris$Species == \u0026quot;setosa\u0026quot;, ],\rbase_1 = subset(iris, Species == \u0026quot;setosa\u0026quot;),\rdplyr = filter(iris, Species == \u0026quot;setosa\u0026quot;),\rtimes = 1e3\r)\rmcb_filter\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 28.2 31.5 39.5817 35.8 47.10 219.6 1000\rbase_1 38.0 44.9 57.0533 54.7 65.00 1620.5 1000\rdplyr 347.7 384.7 420.2611 396.5 414.95 2603.2 1000\rautoplot(mcb_filter) + ggtitle(\u0026quot;Filter: Base R vs dplyr\u0026quot;)\ragg_filter \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_filter), mean)$time\rSo, base R code is on average 11 times faster than dplyr::filter. The absolute difference in 10k simulations is 3.806794 seconds.\n2. Select Columns # Selecting specific columns is fundamental to narrowing down datasets. The comparison here shows how base R indexing and the subset function stack up against dplyr::select in terms of speed.\nmcb_select \u0026lt;- microbenchmark(\rbase_0 = iris[, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Petal.Length\u0026quot;)],\rbase_1 = subset(iris, select = c(Sepal.Length, Petal.Length)),\rdplyr = select(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_select\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 5.8 6.7 10.7370 9.2 10.6 1388.9 1000\rbase_1 25.1 31.7 41.3153 40.9 47.6 1471.5 1000\rdplyr 406.2 436.2 467.0370 448.9 466.2 2545.5 1000\rautoplot(mcb_select) + ggtitle(\u0026quot;Select: Base R vs dplyr\u0026quot;)\ragg_select \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_select), mean)$time\rSo, base R code is on average 43 times faster than dplyr::select. The absolute difference in 10k simulations is 4.563 seconds.\n3. Add/Modify Columns # Adding or modifying columns is crucial for feature engineering. Base R provides multiple methods for this, which are benchmarked here against dplyr::mutate for their computational efficiency.\nmcb_mutate \u0026lt;- microbenchmark(\rbase_0 = {iris$Sepal_LbyW \u0026lt;- iris$Sepal.Length / iris$Sepal.Width},\rbase_1 = transform(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rdplyr = mutate(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rtimes = 1e3\r)\rmcb_mutate\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 3.2 4.40 6.4022 6.25 8.20 25.9 1000\rbase_1 27.3 38.70 50.2744 50.00 58.85 1450.9 1000\rdplyr 360.2 401.05 428.6732 415.65 431.45 1964.5 1000\rautoplot(mcb_mutate) + ggtitle(\u0026quot;Mutate: Base R vs dplyr\u0026quot;)\ragg_mutate \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_mutate), mean)$time\rSo, base R code is on average 67 times faster than dplyr::mutate. The absolute difference in 10k simulations is 4.22271 seconds.\n4. Summarise Data # Data summarization helps derive aggregate metrics. This section compares colMeans from base R with dplyr::summarise_if, illustrating their relative performance in summarizing numeric columns.\nmcb_summarise \u0026lt;- microbenchmark(\rbase_0 = with(iris, data.frame(Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width))),\rdplyr = summarise(iris, Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width)),\rtimes = 1e3\r)\rmcb_summarise\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 117.2 136.3 165.6190 166.40 179.10 3398.5 1000\rdplyr 589.1 651.8 697.3837 674.85 702.95 2235.8 1000\rautoplot(mcb_summarise) + ggtitle(\u0026quot;Summarise: Base R vs dplyr\u0026quot;)\ragg_summarise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_summarise), mean)$time\rSo, base R code is on average 4 times faster than dplyr::summarise. The absolute difference in 10k simulations is 5.317647 seconds.\n5. Grouped Summary # Aggregating data by groups is often required for advanced analytics. Base R’s aggregate function is tested here against dplyr\u0026rsquo;s group_by and summarise_all functions to highlight performance differences.\nmcb_grp \u0026lt;- microbenchmark(\rbase_0 = aggregate(. ~ Species, data = iris, FUN = mean, na.rm = TRUE),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rsummarise_all(mean, na.rm = TRUE) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 638.0 680.80 742.3042 720.70 744.55 2785.4 1000\rdplyr 1935.6 2014.55 2132.6932 2047.95 2110.20 5475.2 1000\rautoplot(mcb_grp) + ggtitle(\u0026quot;Grouped Summarise: Base R vs dplyr\u0026quot;)\ragg_grp \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp), mean)$time\rSo, base R code is on average 3 times faster than dplyr::grp. The absolute difference in 10k simulations is 13.90389 seconds.\n6. Sort Data # Sorting is essential for ordering data before visualization or further analysis. The benchmarks here compare the classic base R order approach to the elegant dplyr::arrange.\nmcb_sort \u0026lt;- microbenchmark(\rbase_0 = iris[with(iris, order(Sepal.Length, Petal.Length)), ],\rdplyr = arrange(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_sort\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 39.4 51.20 67.530 73.2 77.85 1301.3 1000\rdplyr 1296.7 1330.55 1432.962 1348.4 1396.05 39396.9 1000\rautoplot(mcb_sort) + ggtitle(\u0026quot;Sort: Base R vs dplyr\u0026quot;)\ragg_sort \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_sort), mean)$time\rSo, base R code is on average 21 times faster than dplyr::arrange. The absolute difference in 10k simulations is 13.654316 seconds.\n7. Join Data # Data joins are indispensable when working with relational datasets. This section demonstrates the efficiency of base R’s merge function compared to dplyr::left_join.\niris$id \u0026lt;- sample.int(nrow(iris), replace = FALSE)\riris2 \u0026lt;- iris[sample(nrow(iris)),]\rmcb_join \u0026lt;- microbenchmark(\rbase_0 = merge(iris, iris2, by = \u0026quot;id\u0026quot;, all.x = TRUE),\rdplyr = left_join(iris, iris2, by = \u0026quot;id\u0026quot;),\rtimes = 1e3\r)\rmcb_join\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 261.2 303.10 344.9787 337.05 359.15 2439.3 1000\rdplyr 612.9 671.35 718.9914 692.65 718.30 2661.7 1000\rautoplot(mcb_join) + ggtitle(\u0026quot;Join: Base R vs dplyr\u0026quot;)\ragg_join \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_join), mean)$time\rSo, base R code is on average 2 times faster than dplyr::left_join. The absolute difference in 10k simulations is 3.740127 seconds.\n8. Group and Apply Function 1 # Applying models or transformations to groups of data is a frequent task in statistical workflows. This benchmark showcases the performance of split and lapply in base R versus dplyr::group_map.\nmcb_grp_map \u0026lt;- microbenchmark(\rbase_0 = lapply(split(iris, iris$Species), function(x) lm(Sepal.Length ~ Petal.Length, data = x)),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_map(~ lm(Sepal.Length ~ Petal.Length, data = .x)),\rtimes = 1e3\r)\rmcb_grp_map\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 758.0 817.05 887.7093 843.75 870.8 2685.5 1000\rdplyr 2354.5 2422.50 2558.5587 2458.40 2534.1 5709.2 1000\rautoplot(mcb_grp_map) + ggtitle(\u0026quot;Group Map: Base R vs dplyr\u0026quot;)\ragg_grp_map \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_map), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_map. The absolute difference in 10k simulations is 16.708494 seconds.\n9. Group and Apply Function 2 # Complex operations on grouped data are common in analytical pipelines. This comparison highlights the performance of base R’s split and lapply with a row-binding step versus dplyr::group_modify.\nmcb_grp_mod \u0026lt;- microbenchmark(\rbase_0 = do.call(rbind, lapply(split(iris, iris$Species), function(x) data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = x))$coefficients))),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_modify(~ data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = .x))$coefficients)) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp_mod\rUnit: milliseconds\rexpr min lq mean median uq max neval\rbase_0 1.3785 1.47355 1.608932 1.50645 1.5597 40.7367 1000\rdplyr 4.3195 4.47495 4.716570 4.58045 4.7456 8.3596 1000\rautoplot(mcb_grp_mod) + ggtitle(\u0026quot;Group Modify: Base R vs dplyr\u0026quot;)\ragg_grp_mod \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_mod), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_modify. The absolute difference in 10k simulations is 31.076376 seconds.\n10. Rowwise # mcb_rowwise \u0026lt;- microbenchmark(base_0 = transform(iris, Sepal_sd = apply(cbind(Sepal.Length, Sepal.Width), 1, sd),\rPetal_sd = apply(cbind(Petal.Length, Petal.Width), 1, sd)),\rdplyr = iris |\u0026gt;\rdplyr::rowwise() |\u0026gt;\rdplyr::mutate(Sepal_sd = sd(c(Sepal.Length, Sepal.Width)),\rPetal_sd = sd(c(Petal.Length, Petal.Width))) |\u0026gt;\rdplyr::ungroup(),\rtimes = 1e3)\rautoplot(mcb_rowwise) + ggtitle(\u0026quot;Rowwise: Base R vs dplyr\u0026quot;)\ragg_rowwise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_rowwise), mean)$time\rSo, base R code is on average 3 times faster than dplyr::rowwise. The absolute difference in 10k simulations is 21.667426 seconds.\n11. Count Rows by Group # Counting the number of rows by group is a simple yet frequent operation. The comparison here emphasizes the efficiency of base R’s table function against dplyr::count.\nmcb_count \u0026lt;- microbenchmark(\rbase_0 = table(iris$Species),\rdplyr = iris |\u0026gt; count(Species),\rtimes = 1e3\r)\rmcb_count\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 14.9 21.05 32.5562 36.30 39.90 1374.4 1000\rdplyr 1298.7 1348.40 1433.3286 1373.65 1428.55 3183.0 1000\rautoplot(mcb_count) + ggtitle(\u0026quot;Count: Base R vs dplyr\u0026quot;)\ragg_count \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_count), mean)$time\rSo, base R code is on average 44 times faster than dplyr::count. The absolute difference in 10k simulations is 14.007724 seconds.\n12. Identify Distinct Rows # Identifying unique rows is crucial for deduplication. This benchmark compares base R’s unique function with dplyr::distinct, shedding light on performance differences for this operation.\nmcb_distinct \u0026lt;- microbenchmark(\rbase_0 = unique(rbind(iris, iris)),\rdplyr = distinct(rbind(iris, iris)),\rtimes = 1e3\r)\rmcb_distinct\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 564.2 598.0 656.7336 612.90 638.05 2461.0 1000\rdplyr 232.4 261.8 286.6282 274.35 289.75 2160.5 1000\rautoplot(mcb_distinct) + ggtitle(\u0026quot;Distinct: Base R vs dplyr\u0026quot;)\ragg_distinct \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_distinct), mean)$time\rSo, base R code is on average 2 times slower than dplyr::distinct. The absolute difference in 10k simulations is 3.701054 seconds.\nSummary # Given that these functions are very commonly used, it is fair to assume that these functions are used at least 5 times in a standard simulation code. If that simulation is repeated for 10k times, then the total gain we have by using base R is at least 10.7472875 minutes.\nConclusion # This analysis demonstrates\nthat while dplyr offers user-friendly functions and a consistent syntax, base R can often be faster for basic operations, especially with large datasets. The choice between base R and dplyr should consider both readability and computational efficiency, dependent on the scale and complexity of your data tasks.\nFor intensive data simulations, careful function choice can lead to significant performance gains, underscoring the importance of benchmarking and profiling in the R language. "},{"id":2,"href":"/about/","title":"About","section":"Welcome","content":" About Me # Welcome to my blog!\nI’m an experienced statistician and an avid R enthusiast. Over the years, R has become an integral part of my professional and personal toolkit. Whether it’s performing advanced statistical modeling, developing data visualizations, or exploring machine learning algorithms, I believe R empowers users to push the boundaries of data analysis.\nThis blog is my humble attempt to share what I’ve learned and continue to learn. Here, you’ll find tutorials, tips, and insights about:\nR programming best practices Statistical methods and modeling Data visualization techniques Machine learning in R Performance optimization and comparisons Tools like Shiny, ggplot2, and more I’ll also share examples from real-world scenarios, such as clinical trial data analysis and simulation studies, where R has proven to be invaluable.\nIf you’re passionate about data science, programming, or statistics—or just starting your R journey—you’re in the right place. Let’s grow and learn together, one R script at a time!\nThank you for stopping by, and happy coding!\n"}]