[{"id":0,"href":"/post/2025/04/18/bnns-binary-classification/","title":"Predicting Diabetes with Bayesian Neural Networks: More Than Just a Probability","section":"Posts","content":" In critical applications like healthcare, knowing the probability isn‚Äôt enough. Knowing how confident we are in that probability makes all the difference.\nüîç The Problem # Diabetes prediction models abound, but they often give us just a number. Traditional classifiers like logistic regression or random forests will tell you, for instance, that there\u0026rsquo;s a 73% chance someone has diabetes.\nBut what if we asked:\n\u0026ldquo;How sure are we about that 73%?\u0026rdquo;\nEnter Bayesian Neural Networks (bnns), where uncertainty isn‚Äôt a bug‚Äîit‚Äôs a feature.\nüìä The Data # We used the PimaIndiansDiabetes2 dataset from the mlbench package, containing clinical variables (e.g., glucose, BMI, insulin) and diabetes diagnosis. Missing values were removed, and outcomes were converted to binary.\nlibrary(bnns)\rlibrary(mlbench)\rlibrary(rsample)\rlibrary(ggplot2)\rset.seed(123)\rdata(\u0026quot;PimaIndiansDiabetes2\u0026quot;)\rtrial_data \u0026lt;- PimaIndiansDiabetes2 |\u0026gt;\rtransform(diabetes = ifelse(diabetes == \u0026quot;pos\u0026quot;, 1, 0)) |\u0026gt;\rna.omit()\rtrial_data_split \u0026lt;- initial_split(trial_data, strata = diabetes, prop = 0.8)\rtrial_data_train \u0026lt;- training(trial_data_split)\rtrial_data_test \u0026lt;- testing(trial_data_split)\rüß† The Model # Our BNN has two hidden layers (24 and 12 nodes) and uses Cauchy priors to encourage sparsity. We used logistic output activation (out_act_fn = 2) for binary classification.\nbnn_model \u0026lt;- bnns(\rdiabetes ~ .,\rdata = trial_data_train,\rL = 2,\rnodes = c(24, 12),\ract_fn = c(4, 1),\rout_act_fn = 2,\riter = 0.1e4,\rwarmup = 0.05e4,\rchains = 4,\rcores = 4,\rprior_weights = list(dist = \u0026quot;cauchy\u0026quot;, params = list(mu = 0, sigma = 0.3)),\rprior_bias = list(dist = \u0026quot;cauchy\u0026quot;, params = list(mu = 0, sigma = 0.3))\r)\rüéØ The Prediction # We predicted probabilities on the test set and extracted the posterior median and 95% credible intervals.\npred \u0026lt;- predict(bnn_model, newdata = trial_data_test)\rpred_y \u0026lt;- apply(pred, MARGIN = 1, median)\rpred_quantiles \u0026lt;- apply(pred, 1, function(x) quantile(x, probs = c(0.025, 0.975)), simplify = FALSE) |\u0026gt;\rdo.call(rbind, args = _)\rüìè Model Performance # measure_bin(trial_data_test$diabetes, pred)\rSetting levels: control = 0, case = 1\rSetting direction: controls \u0026lt; cases\r$conf_mat\rpred_label\robs 0 1\r0 42 11\r1 7 19\r$accuracy\r[1] 0.7721519\r$ROC\rCall:\rroc.default(response = obs, predictor = pred)\rData: pred in 53 controls (obs 0) \u0026lt; 26 cases (obs 1).\rArea under the curve: 0.8353\r$AUC\r[1] 0.8352685\rPerformance was comparable to classical methods (AUC ~0.835), but with a massive advantage: uncertainty quantification.\nüî¨ Visualizing Predictions # plot_data \u0026lt;- data.frame(\rActual = trial_data_test$diabetes,\rPredicted = pred_y,\rLower = pred_quantiles[,1],\rUpper = pred_quantiles[,2]\r) |\u0026gt;\rtransform(width = Upper - Lower) |\u0026gt;\rdplyr::arrange(width)\rggplot(plot_data, aes(x = Actual, y = Predicted)) +\rgeom_point(size = 2) +\rgeom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.05, color = \u0026quot;steelblue\u0026quot;) +\rlabs(\rtitle = \u0026quot;BNN Predictions for Patient Status\u0026quot;,\rsubtitle = \u0026quot;Error bars show 95% credible intervals\u0026quot;,\rx = \u0026quot;Test Set Patient Status\u0026quot;,\ry = \u0026quot;Predicted Patient Status\u0026quot;\r) +\rtheme_minimal()\rüß† Why Use bnns? # Non-linear modeling: Captures complex relationships among covariates without heavy pre-specification. Credible intervals: Each prediction comes with its uncertainty. Probability of a probability: For example: Pr(Pr(Diabetes) \u0026gt; 0.5) \u0026gt; 0.9 Translation: \u0026ldquo;We are 90% confident this person has more than 50% risk.\u0026rdquo; That‚Äôs clinical-grade insight. üì£ Final Thoughts # Bayesian Neural Networks let us move beyond binary thinking and embrace informed uncertainty. When the stakes are high (think medicine, finance, or policy), bnns offers a probabilistic flashlight in the black box jungle.\nReady to stop guessing your predictions? Start modeling your belief about your predictions.\nüîó Explore the bnns package\nüí¨ Like this blog? Share your thoughts and use-cases on LinkedIn and tag me!\n"},{"id":1,"href":"/post/2025/01/17/bnns-for-tmle/","title":"Using bnns for TMLE","section":"Posts","content":" Introduction # This document demonstrates how to use the bnns package with TMLE for causal inference. TMLE combines machine learning-based flexible models with statistical principles to produce unbiased and efficient estimators of causal effects, such as the Average Treatment Effect (ATE). The example also highlights how the flexibility of Bayesian Neural Networks (BNNs) can improve TMLE results when handling complex data-generating mechanisms. This tutorial borrows from this tmle tutorial.\nSimulating Data # We simulate data where the treatment assignment and outcome are influenced by multiple covariates, and the true ATE is known.\nlibrary(bnns)\r# Set a random seed for reproducibility\rset.seed(123)\r# Simulate data\rn \u0026lt;- 1000 # Number of samples\rX1 \u0026lt;- rnorm(n)\rX2 \u0026lt;- rnorm(n)\rU \u0026lt;- rbinom(n, 1, 0.5) # Unmeasured confounder\r# Treatment mechanism (biased by U)\rps \u0026lt;- plogis(0.5 * X1 - 0.5 * X2 + 0.8 * U)\rA \u0026lt;- rbinom(n, 1, ps) # Treatment assignment\r# Outcome mechanism (non-linear and confounded by U)\rY_prob \u0026lt;- plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * A + 1.5 * U)\rY \u0026lt;- rbinom(n, 1, Y_prob)\r# Combine into a dataset\rsim_data \u0026lt;- data.frame(Y, A, X1, X2)\r# True ATE for comparison\rtrue_ate \u0026lt;- mean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 1 + 1.5 * U)) -\rmean(plogis(1 + 2 * X1^2 - 1.5 * X2 + 1.2 * 0 + 1.5 * U))\rtrue_ate\r#\u0026gt; [1] 0.07431068\rApplying TMLE with bnns # Step 1: Install and Load Required Packages # Ensure that the bnns and tmle packages are installed.\n# Install ggplot2 and bnns from CRAN\rinstall.packages(\u0026quot;ggplot2\u0026quot;)\rinstall.packages(\u0026quot;bnns\u0026quot;)\rStep 2: TMLE Implementation # We use tmle to estimate the ATE. Both the treatment mechanism (g) and the outcome mechanism (Q) are modeled using Bayesian Neural Networks via the bnns package.\n### Step 1: Estimate Q\rQ \u0026lt;- bnns(Y ~ -1 + ., data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rQ_A \u0026lt;- predict(Q) # obtain predictions for everyone using the treatment they actually received\rsim_data_1 \u0026lt;- sim_data |\u0026gt; transform(A = 1) # data set where everyone received treatment\rQ_1 \u0026lt;- predict(Q, newdata = sim_data_1) # predict on that everyone-exposed data set\rsim_data_0 \u0026lt;- sim_data |\u0026gt; transform(A = 0) # data set where no one received treatment\rQ_0 \u0026lt;- predict(Q, newdata = sim_data_0)\rdat_tmle \u0026lt;- lapply(1:dim(Q_A)[2], function(i) data.frame(Y = sim_data$Y, A = sim_data$A, Q_A = Q_A[,i], Q_0 = Q_0[,i], Q_1 = Q_1[,i]))\r### Step 2: Estimate g and compute H(A,W)\rg \u0026lt;- bnns(A ~ -1 + . - Y, data = sim_data, L = 2, nodes = c(2, 2), act_fn = c(2, 2), out_act_fn = 2)\rg_w \u0026lt;- predict(g) # Pr(A=1|W)\rH_1 \u0026lt;- 1/g_w\rH_0 \u0026lt;- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)\rdat_tmle \u0026lt;- # add clever covariate data to dat_tmle\rlapply(1:dim(Q_A)[2], function(i) dat_tmle[[i]] |\u0026gt;\rcbind(\rH_1 = H_1[,i],\rH_0 = H_0[,i]) |\u0026gt;\rtransform(H_A = ifelse(A == 1, H_1, # if A is 1 (treated), assign H_1\rH_0)) # if A is 0 (not treated), assign H_0\r)\r### Step 3: Estimate fluctuation parameter\rtmle_ate_list \u0026lt;- lapply(1:dim(Q_A)[2], function(i){\rglm_fit \u0026lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle[[i]], family=binomial) # fixed intercept logistic regression\reps \u0026lt;- coef(glm_fit) # save the only coefficient, called epsilon in TMLE lit\r### Step 4: Update Q's\rQ_A_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_A) + eps*H_A)) # updated expected outcome given treatment actually received\rQ_1_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_1) + eps*H_1)) # updated expected outcome for everyone receiving treatment\rQ_0_update \u0026lt;- with(dat_tmle[[i]], plogis(qlogis(Q_0) + eps*H_0)) # updated expected outcome for everyone not receiving treatment\r### Step 5: Compute ATE\rtmle_ate \u0026lt;- mean(Q_1_update - Q_0_update) # mean diff in updated expected outcome estimates return(tmle_ate)\r})\rtmle_ate \u0026lt;- unlist(tmle_ate_list)\rmedian(tmle_ate)\r#\u0026gt; [1] 0.08956612\rComparing TMLE to Traditional Methods # To highlight the benefits of TMLE, compare it to other methods such as:\nIPTW (Inverse Probability of Treatment Weighting) Naive Comparison (Unadjusted Difference in Means) # IPTW\r# ps_model \u0026lt;- glm(A ~ .-Y, data = sim_data, family = binomial)\r# ps_pred \u0026lt;- predict(ps_model, type = \u0026quot;response\u0026quot;)\r# iptw_weights \u0026lt;- ifelse(sim_data$A == 1, 1 / ps_pred, 1 / (1 - ps_pred))\r# iptw_ate \u0026lt;- mean(iptw_weights * sim_data$Y * sim_data$A) -\r# mean(iptw_weights * sim_data$Y * (1 - sim_data$A))\rlibrary(tmle)\r#\u0026gt; Loading required package: glmnet\r#\u0026gt; Loading required package: Matrix\r#\u0026gt; Loaded glmnet 4.1-8\r#\u0026gt; Loading required package: SuperLearner\r#\u0026gt; Loading required package: nnls\r#\u0026gt; Loading required package: gam\r#\u0026gt; Loading required package: splines\r#\u0026gt; Loading required package: foreach\r#\u0026gt; Loaded gam 1.22-5\r#\u0026gt; Super Learner\r#\u0026gt; Version: 2.0-29\r#\u0026gt; Package created on 2024-02-06\r#\u0026gt; Welcome to the tmle package, version 2.0.1.1\r#\u0026gt; #\u0026gt; Use tmleNews() to see details on changes and bug fixes\rfreq_tmle \u0026lt;- tmle(\rY = sim_data$Y, A = sim_data$A, W = sim_data[, c(\u0026quot;X1\u0026quot;, \u0026quot;X2\u0026quot;)], Q.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;),\rg.SL.library = c(\u0026quot;SL.glm\u0026quot;, \u0026quot;SL.ranger\u0026quot;)\r)\r#\u0026gt; Loading required namespace: ranger\r# Results\rfreq_tmle_ate \u0026lt;- freq_tmle$estimates$ATE$psi\r# Naive Comparison\rnaive_ate \u0026lt;- mean(sim_data$Y[sim_data$A == 1]) - mean(sim_data$Y[sim_data$A == 0])\r# Summary of Results\rresults \u0026lt;- data.frame(\rMethod = c(\u0026quot;True ATE\u0026quot;, \u0026quot;BNN_TMLE\u0026quot;, \u0026quot;Freq_TMLE\u0026quot;, \u0026quot;Naive\u0026quot;),\rEstimate = c(true_ate, median(tmle_ate), freq_tmle_ate, naive_ate),\rCI_low = c(true_ate, quantile(tmle_ate, probs = c(0.025)), freq_tmle$estimates$ATE$CI[1], naive_ate),\rCI_high = c(true_ate, quantile(tmle_ate, probs = c(0.975)), freq_tmle$estimates$ATE$CI[2], naive_ate)\r)\rresults\r#\u0026gt; Method Estimate CI_low CI_high\r#\u0026gt; 1 True ATE 0.07431068 0.07431068 0.07431068\r#\u0026gt; 2 BNN_TMLE 0.08956612 0.08372502 0.09625603\r#\u0026gt; 3 Freq_TMLE 0.09309979 0.05910546 0.12709413\r#\u0026gt; 4 Naive 0.12824940 0.12824940 0.12824940\rVisualizing Results # Plot the estimates from different methods to compare their performance.\nlibrary(ggplot2)\rggplot(results, aes(x = forcats::fct_reorder(Method, Estimate), y = Estimate, fill = Method)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, color = \u0026quot;black\u0026quot;) +\rgeom_hline(yintercept = true_ate, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) +\rgeom_errorbar(mapping = aes(ymin = CI_low, ymax = CI_high)) +\rlabs(\rtitle = \u0026quot;Comparison of ATE Estimates\u0026quot;,\ry = \u0026quot;ATE Estimate\u0026quot;,\rx = \u0026quot;Method\u0026quot;\r) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\rSummary # This tutorial demonstrates how to use the bnns package for TMLE to estimate the Average Treatment Effect (ATE). By leveraging the flexibility of Bayesian Neural Networks, TMLE can handle complex data structures and improve accuracy compared to traditional methods. Use this workflow for applications in clinical trials, epidemiology, and other domains requiring robust causal inference.\n"},{"id":2,"href":"/post/2025/01/17/power-of-base-r/","title":"Power of Base R: A Performance Comparison with dplyr","section":"Posts","content":" Introduction # This presentation explores the performance differences between base R and the dplyr package for various data manipulation tasks.\nWhile dplyr is renowned for its intuitive syntax and efficiency, base R functions can sometimes outperform it, particularly in large simulations. Understanding these differences can aid in making informed decisions when choosing data wrangling techniques.\nThe Iris Dataset # The iris dataset is a classic dataset in statistics and machine learning, often used for demonstrating data manipulation and analysis techniques.\nIt contains measurements of flower characteristics for three species of iris: setosa, versicolor, and virginica.\nKey Features:\n150 observations: 50 samples for each species.\n4 numeric attributes: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n1 categorical attribute: Species: The species of the iris flower (setosa, versicolor, virginica).\nThe Iris Dataset (continued) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r1 5.1 3.5 1.4 0.2 setosa\r2 4.9 3.0 1.4 0.2 setosa\r3 4.7 3.2 1.3 0.2 setosa\r51 7.0 3.2 4.7 1.4 versicolor\r52 6.4 3.2 4.5 1.5 versicolor\r53 6.9 3.1 4.9 1.5 versicolor\r101 6.3 3.3 6.0 2.5 virginica\r102 5.8 2.7 5.1 1.9 virginica\r103 7.1 3.0 5.9 2.1 virginica\rBenchmarking Data Manipulation Tasks # 1. Filter Rows # Filtering rows is a common operation in data analysis. This section benchmarks the efficiency of base R‚Äôs subsetting approaches against the dplyr::filter function. While dplyr provides clean syntax, base R‚Äôs performance is more efficient.\nmcb_filter \u0026lt;- microbenchmark(\rbase_0 = iris[iris$Species == \u0026quot;setosa\u0026quot;, ],\rbase_1 = subset(iris, Species == \u0026quot;setosa\u0026quot;),\rdplyr = filter(iris, Species == \u0026quot;setosa\u0026quot;),\rtimes = 1e3\r)\rmcb_filter\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 27.8 32.40 40.5975 37.40 47.70 273.3 1000\rbase_1 37.8 46.70 57.1885 55.70 65.60 167.5 1000\rdplyr 360.9 392.25 435.6562 405.75 432.65 4076.9 1000\rautoplot(mcb_filter) + ggtitle(\u0026quot;Filter: Base R vs dplyr\u0026quot;)\ragg_filter \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_filter), mean)$time\rSo, base R code is on average 11 times faster than dplyr::filter. The absolute difference in 10k simulations is 3.950587 seconds.\n2. Select Columns # Selecting specific columns is fundamental to narrowing down datasets. The comparison here shows how base R indexing and the subset function stack up against dplyr::select in terms of speed.\nmcb_select \u0026lt;- microbenchmark(\rbase_0 = iris[, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Petal.Length\u0026quot;)],\rbase_1 = subset(iris, select = c(Sepal.Length, Petal.Length)),\rdplyr = select(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_select\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 5.7 6.5 10.3490 9.10 10.40 1422.1 1000\rbase_1 24.9 31.9 41.6527 41.70 47.60 341.7 1000\rdplyr 409.0 441.9 483.2336 455.05 473.65 2125.1 1000\rautoplot(mcb_select) + ggtitle(\u0026quot;Select: Base R vs dplyr\u0026quot;)\ragg_select \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_select), mean)$time\rSo, base R code is on average 47 times faster than dplyr::select. The absolute difference in 10k simulations is 4.728846 seconds.\n3. Add/Modify Columns # Adding or modifying columns is crucial for feature engineering. Base R provides multiple methods for this, which are benchmarked here against dplyr::mutate for their computational efficiency.\nmcb_mutate \u0026lt;- microbenchmark(\rbase_0 = {iris$Sepal_LbyW \u0026lt;- iris$Sepal.Length / iris$Sepal.Width},\rbase_1 = transform(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rdplyr = mutate(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rtimes = 1e3\r)\rmcb_mutate\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 3.2 4.20 6.4048 6.80 8.00 31.1 1000\rbase_1 26.5 38.25 48.2512 47.95 56.65 133.4 1000\rdplyr 368.3 393.20 428.1848 406.45 423.45 1933.3 1000\rautoplot(mcb_mutate) + ggtitle(\u0026quot;Mutate: Base R vs dplyr\u0026quot;)\ragg_mutate \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_mutate), mean)$time\rSo, base R code is on average 67 times faster than dplyr::mutate. The absolute difference in 10k simulations is 4.2178 seconds.\n4. Summarise Data # Data summarization helps derive aggregate metrics. This section compares colMeans from base R with dplyr::summarise_if, illustrating their relative performance in summarizing numeric columns.\nmcb_summarise \u0026lt;- microbenchmark(\rbase_0 = with(iris, data.frame(Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width))),\rdplyr = summarise(iris, Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width)),\rtimes = 1e3\r)\rmcb_summarise\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 114.2 134.65 159.2314 167.05 176.3 652.6 1000\rdplyr 610.1 658.20 708.2480 674.30 693.4 3743.6 1000\rautoplot(mcb_summarise) + ggtitle(\u0026quot;Summarise: Base R vs dplyr\u0026quot;)\ragg_summarise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_summarise), mean)$time\rSo, base R code is on average 4 times faster than dplyr::summarise. The absolute difference in 10k simulations is 5.490166 seconds.\n5. Grouped Summary # Aggregating data by groups is often required for advanced analytics. Base R‚Äôs aggregate function is tested here against dplyr\u0026rsquo;s group_by and summarise_all functions to highlight performance differences.\nmcb_grp \u0026lt;- microbenchmark(\rbase_0 = aggregate(. ~ Species, data = iris, FUN = mean, na.rm = TRUE),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rsummarise_all(mean, na.rm = TRUE) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 644.7 700.40 756.0318 732.20 749.7 2721.7 1000\rdplyr 1974.1 2038.25 2166.2883 2062.65 2127.0 11647.2 1000\rautoplot(mcb_grp) + ggtitle(\u0026quot;Grouped Summarise: Base R vs dplyr\u0026quot;)\ragg_grp \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp), mean)$time\rSo, base R code is on average 3 times faster than dplyr::grp. The absolute difference in 10k simulations is 14.102565 seconds.\n6. Sort Data # Sorting is essential for ordering data before visualization or further analysis. The benchmarks here compare the classic base R order approach to the elegant dplyr::arrange.\nmcb_sort \u0026lt;- microbenchmark(\rbase_0 = iris[with(iris, order(Sepal.Length, Petal.Length)), ],\rdplyr = arrange(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_sort\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 39.6 51.3 66.4787 72.05 76.2 1177.0 1000\rdplyr 1290.4 1349.6 1461.4935 1364.75 1405.9 39121.2 1000\rautoplot(mcb_sort) + ggtitle(\u0026quot;Sort: Base R vs dplyr\u0026quot;)\ragg_sort \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_sort), mean)$time\rSo, base R code is on average 22 times faster than dplyr::arrange. The absolute difference in 10k simulations is 13.950148 seconds.\n7. Join Data # Data joins are indispensable when working with relational datasets. This section demonstrates the efficiency of base R‚Äôs merge function compared to dplyr::left_join.\niris$id \u0026lt;- sample.int(nrow(iris), replace = FALSE)\riris2 \u0026lt;- iris[sample(nrow(iris)),]\rmcb_join \u0026lt;- microbenchmark(\rbase_0 = merge(iris, iris2, by = \u0026quot;id\u0026quot;, all.x = TRUE),\rdplyr = left_join(iris, iris2, by = \u0026quot;id\u0026quot;),\rtimes = 1e3\r)\rmcb_join\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 254.7 299.6 349.5052 335.75 346.4 10593.6 1000\rdplyr 625.5 660.9 710.3915 676.35 698.6 2619.2 1000\rautoplot(mcb_join) + ggtitle(\u0026quot;Join: Base R vs dplyr\u0026quot;)\ragg_join \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_join), mean)$time\rSo, base R code is on average 2 times faster than dplyr::left_join. The absolute difference in 10k simulations is 3.608863 seconds.\n8. Group and Apply Function 1 # Applying models or transformations to groups of data is a frequent task in statistical workflows. This benchmark showcases the performance of split and lapply in base R versus dplyr::group_map.\nmcb_grp_map \u0026lt;- microbenchmark(\rbase_0 = lapply(split(iris, iris$Species), function(x) lm(Sepal.Length ~ Petal.Length, data = x)),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_map(~ lm(Sepal.Length ~ Petal.Length, data = .x)),\rtimes = 1e3\r)\rmcb_grp_map\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 775.7 839.55 912.2631 858.85 890.8 3862.9 1000\rdplyr 2363.6 2443.75 2576.9940 2480.70 2560.0 4558.9 1000\rautoplot(mcb_grp_map) + ggtitle(\u0026quot;Group Map: Base R vs dplyr\u0026quot;)\ragg_grp_map \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_map), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_map. The absolute difference in 10k simulations is 16.647309 seconds.\n9. Group and Apply Function 2 # Complex operations on grouped data are common in analytical pipelines. This comparison highlights the performance of base R‚Äôs split and lapply with a row-binding step versus dplyr::group_modify.\nmcb_grp_mod \u0026lt;- microbenchmark(\rbase_0 = do.call(rbind, lapply(split(iris, iris$Species), function(x) data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = x))$coefficients))),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_modify(~ data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = .x))$coefficients)) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp_mod\rUnit: milliseconds\rexpr min lq mean median uq max neval\rbase_0 1.3825 1.49395 1.604264 1.5339 1.59905 6.1623 1000\rdplyr 4.3550 4.52505 4.843361 4.6633 4.82740 42.1022 1000\rautoplot(mcb_grp_mod) + ggtitle(\u0026quot;Group Modify: Base R vs dplyr\u0026quot;)\ragg_grp_mod \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_mod), mean)$time\rSo, base R code is on average 3 times faster than dplyr::group_modify. The absolute difference in 10k simulations is 32.390974 seconds.\n10. Rowwise # mcb_rowwise \u0026lt;- microbenchmark(base_0 = transform(iris, Sepal_sd = apply(cbind(Sepal.Length, Sepal.Width), 1, sd),\rPetal_sd = apply(cbind(Petal.Length, Petal.Width), 1, sd)),\rdplyr = iris |\u0026gt;\rdplyr::rowwise() |\u0026gt;\rdplyr::mutate(Sepal_sd = sd(c(Sepal.Length, Sepal.Width)),\rPetal_sd = sd(c(Petal.Length, Petal.Width))) |\u0026gt;\rdplyr::ungroup(),\rtimes = 1e3)\rautoplot(mcb_rowwise) + ggtitle(\u0026quot;Rowwise: Base R vs dplyr\u0026quot;)\ragg_rowwise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_rowwise), mean)$time\rSo, base R code is on average 3 times faster than dplyr::rowwise. The absolute difference in 10k simulations is 21.147045 seconds.\n11. Count Rows by Group # Counting the number of rows by group is a simple yet frequent operation. The comparison here emphasizes the efficiency of base R‚Äôs table function against dplyr::count.\nmcb_count \u0026lt;- microbenchmark(\rbase_0 = table(iris$Species),\rdplyr = iris |\u0026gt; count(Species),\rtimes = 1e3\r)\rmcb_count\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 15.1 20.95 30.1702 35.8 38.00 115.8 1000\rdplyr 1283.8 1324.15 1418.8261 1357.9 1406.35 3043.2 1000\rautoplot(mcb_count) + ggtitle(\u0026quot;Count: Base R vs dplyr\u0026quot;)\ragg_count \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_count), mean)$time\rSo, base R code is on average 47 times faster than dplyr::count. The absolute difference in 10k simulations is 13.886559 seconds.\n12. Identify Distinct Rows # Identifying unique rows is crucial for deduplication. This benchmark compares base R‚Äôs unique function with dplyr::distinct, shedding light on performance differences for this operation.\nmcb_distinct \u0026lt;- microbenchmark(\rbase_0 = unique(rbind(iris, iris)),\rdplyr = distinct(rbind(iris, iris)),\rtimes = 1e3\r)\rmcb_distinct\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 568.7 599.7 659.1403 615.2 646.40 3656.1 1000\rdplyr 228.3 256.8 290.7550 270.5 284.05 1908.9 1000\rautoplot(mcb_distinct) + ggtitle(\u0026quot;Distinct: Base R vs dplyr\u0026quot;)\ragg_distinct \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_distinct), mean)$time\rSo, base R code is on average 2 times slower than dplyr::distinct. The absolute difference in 10k simulations is 3.683853 seconds.\nSummary # Given that these functions are very commonly used, it is fair to assume that these functions are used at least 5 times in a standard simulation code. If that simulation is repeated for 10k times, then the total gain we have by using base R is at least 10.8697507 minutes.\nConclusion # This analysis demonstrates\nthat while dplyr offers user-friendly functions and a consistent syntax, base R can often be faster for basic operations, especially with large datasets. The choice between base R and dplyr should consider both readability and computational efficiency, dependent on the scale and complexity of your data tasks.\nFor intensive data simulations, careful function choice can lead to significant performance gains, underscoring the importance of benchmarking and profiling in the R language. "},{"id":3,"href":"/about/","title":"About","section":"Welcome","content":" About Me # Welcome to my blog!\nI‚Äôm an experienced statistician and an avid R enthusiast. Over the years, R has become an integral part of my professional and personal toolkit. Whether it‚Äôs performing advanced statistical modeling, developing data visualizations, or exploring machine learning algorithms, I believe R empowers users to push the boundaries of data analysis.\nThis blog is my humble attempt to share what I‚Äôve learned and continue to learn. Here, you‚Äôll find tutorials, tips, and insights about:\nR programming best practices Statistical methods and modeling Data visualization techniques Machine learning in R Performance optimization and comparisons Tools like Shiny, ggplot2, and more I‚Äôll also share examples from real-world scenarios, such as clinical trial data analysis and simulation studies, where R has proven to be invaluable.\nIf you‚Äôre passionate about data science, programming, or statistics‚Äîor just starting your R journey‚Äîyou‚Äôre in the right place. Let‚Äôs grow and learn together, one R script at a time!\nThank you for stopping by, and happy coding!\n"}]