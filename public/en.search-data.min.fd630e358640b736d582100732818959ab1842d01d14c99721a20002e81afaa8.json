[{"id":0,"href":"/2025/01/17/power-of-base-r-a-performance-comparison-with-dplyr/","title":"Power of Base R: A Performance Comparison with dplyr","section":"Posts","content":" Introduction # This presentation explores the performance differences between base R and the dplyr package for various data manipulation tasks.\nWhile dplyr is renowned for its intuitive syntax and efficiency, base R functions can sometimes outperform it, particularly in large simulations. Understanding these differences can aid in making informed decisions when choosing data wrangling techniques.\nThe Iris Dataset # The iris dataset is a classic dataset in statistics and machine learning, often used for demonstrating data manipulation and analysis techniques.\nIt contains measurements of flower characteristics for three species of iris: setosa, versicolor, and virginica.\nKey Features:\n150 observations: 50 samples for each species.\n4 numeric attributes: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n1 categorical attribute: Species: The species of the iris flower (setosa, versicolor, virginica).\nThe Iris Dataset (continued) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r1 5.1 3.5 1.4 0.2 setosa\r2 4.9 3.0 1.4 0.2 setosa\r3 4.7 3.2 1.3 0.2 setosa\r51 7.0 3.2 4.7 1.4 versicolor\r52 6.4 3.2 4.5 1.5 versicolor\r53 6.9 3.1 4.9 1.5 versicolor\r101 6.3 3.3 6.0 2.5 virginica\r102 5.8 2.7 5.1 1.9 virginica\r103 7.1 3.0 5.9 2.1 virginica\rBenchmarking Data Manipulation Tasks # 1. Filter Rows # Filtering rows is a common operation in data analysis. This section benchmarks the efficiency of base R’s subsetting approaches against the dplyr::filter function. While dplyr provides clean syntax, base R’s performance is more efficient.\nmcb_filter \u0026lt;- microbenchmark(\rbase_0 = iris[iris$Species == \u0026quot;setosa\u0026quot;, ],\rbase_1 = subset(iris, Species == \u0026quot;setosa\u0026quot;),\rdplyr = filter(iris, Species == \u0026quot;setosa\u0026quot;),\rtimes = 1e3\r)\rmcb_filter\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 28.8 34.05 42.4809 38.70 50.30 166.7 1000\rbase_1 39.1 48.65 62.2565 58.75 70.00 2088.4 1000\rdplyr 354.0 409.20 453.3465 427.65 450.95 3167.6 1000\rautoplot(mcb_filter) + ggtitle(\u0026quot;Filter: Base R vs dplyr\u0026quot;)\ragg_filter \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_filter), mean)$time\rSo, base R code is on average r max(round(max(agg_filter)/agg_filter)) times faster than dplyr::filter. The absolute difference in 10k simulations is r (max(agg_filter) - min(agg_filter))*1e-5 seconds.\n2. Select Columns # Selecting specific columns is fundamental to narrowing down datasets. The comparison here shows how base R indexing and the subset function stack up against dplyr::select in terms of speed.\nmcb_select \u0026lt;- microbenchmark(\rbase_0 = iris[, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Petal.Length\u0026quot;)],\rbase_1 = subset(iris, select = c(Sepal.Length, Petal.Length)),\rdplyr = select(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_select\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 5.8 7.2 9.5698 9.80 11.30 102.3 1000\rbase_1 26.2 34.0 45.6122 45.00 51.70 1938.3 1000\rdplyr 409.7 463.0 510.6560 481.25 504.95 4133.6 1000\rautoplot(mcb_select) + ggtitle(\u0026quot;Select: Base R vs dplyr\u0026quot;)\ragg_select \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_select), mean)$time\rSo, base R code is on average r max(round(max(agg_select)/agg_select)) times faster than dplyr::select. The absolute difference in 10k simulations is r (max(agg_select) - min(agg_select))*1e-5 seconds.\n3. Add/Modify Columns # Adding or modifying columns is crucial for feature engineering. Base R provides multiple methods for this, which are benchmarked here against dplyr::mutate for their computational efficiency.\nmcb_mutate \u0026lt;- microbenchmark(\rbase_0 = {iris$Sepal_LbyW \u0026lt;- iris$Sepal.Length / iris$Sepal.Width},\rbase_1 = transform(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rdplyr = mutate(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),\rtimes = 1e3\r)\rmcb_mutate\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 3.3 4.40 6.8406 6.75 8.70 29.4 1000\rbase_1 27.3 38.80 51.1649 51.60 62.15 150.5 1000\rdplyr 376.7 417.75 456.0543 437.45 458.70 2085.8 1000\rautoplot(mcb_mutate) + ggtitle(\u0026quot;Mutate: Base R vs dplyr\u0026quot;)\rSo, base R code is on average r max(round(max(agg_mutate)/agg_mutate)) times faster than dplyr::mutate. The absolute difference in 10k simulations is r (max(agg_mutate) - min(agg_mutate))*1e-5 seconds.\n4. Summarise Data # Data summarization helps derive aggregate metrics. This section compares colMeans from base R with dplyr::summarise_if, illustrating their relative performance in summarizing numeric columns.\nmcb_summarise \u0026lt;- microbenchmark(\rbase_0 = with(iris, data.frame(Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width))),\rdplyr = summarise(iris, Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),\rPetal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width)),\rtimes = 1e3\r)\rmcb_summarise\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 116.5 140.20 169.8306 169.9 184.5 1616.5 1000\rdplyr 594.2 659.35 722.3735 686.3 728.9 4322.8 1000\rautoplot(mcb_summarise) + ggtitle(\u0026quot;Summarise: Base R vs dplyr\u0026quot;)\ragg_summarise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_summarise), mean)$time\rSo, base R code is on average r max(round(max(agg_summarise)/agg_summarise)) times faster than dplyr::summarise. The absolute difference in 10k simulations is r (max(agg_summarise) - min(agg_summarise))*1e-5 seconds.\n5. Grouped Summary # Aggregating data by groups is often required for advanced analytics. Base R’s aggregate function is tested here against dplyr\u0026rsquo;s group_by and summarise_all functions to highlight performance differences.\nmcb_grp \u0026lt;- microbenchmark(\rbase_0 = aggregate(. ~ Species, data = iris, FUN = mean, na.rm = TRUE),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rsummarise_all(mean, na.rm = TRUE) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 641.5 691.40 763.2936 724.15 763.75 3129.5 1000\rdplyr 1951.8 2008.05 2183.1429 2074.10 2184.20 6360.7 1000\rautoplot(mcb_grp) + ggtitle(\u0026quot;Grouped Summarise: Base R vs dplyr\u0026quot;)\rSo, base R code is on average r max(round(max(agg_grp)/agg_grp)) times faster than dplyr::grp. The absolute difference in 10k simulations is r (max(agg_grp) - min(agg_grp))*1e-5 seconds.\n6. Sort Data # Sorting is essential for ordering data before visualization or further analysis. The benchmarks here compare the classic base R order approach to the elegant dplyr::arrange.\nmcb_sort \u0026lt;- microbenchmark(\rbase_0 = iris[with(iris, order(Sepal.Length, Petal.Length)), ],\rdplyr = arrange(iris, Sepal.Length, Petal.Length),\rtimes = 1e3\r)\rmcb_sort\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 40.2 51.50 67.7192 72.10 81.70 245.6 1000\rdplyr 1287.5 1351.35 1483.2003 1392.05 1458.25 37819.6 1000\rautoplot(mcb_sort) + ggtitle(\u0026quot;Sort: Base R vs dplyr\u0026quot;)\ragg_sort \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_sort), mean)$time\rSo, base R code is on average r max(round(max(agg_sort)/agg_sort)) times faster than dplyr::arrange. The absolute difference in 10k simulations is r (max(agg_sort) - min(agg_sort))*1e-5 seconds.\n7. Join Data # Data joins are indispensable when working with relational datasets. This section demonstrates the efficiency of base R’s merge function compared to dplyr::left_join.\niris$id \u0026lt;- sample.int(nrow(iris), replace = FALSE)\riris2 \u0026lt;- iris[sample(nrow(iris)),]\rmcb_join \u0026lt;- microbenchmark(\rbase_0 = merge(iris, iris2, by = \u0026quot;id\u0026quot;, all.x = TRUE),\rdplyr = left_join(iris, iris2, by = \u0026quot;id\u0026quot;),\rtimes = 1e3\r)\rmcb_join\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 259.6 307.85 366.1731 340.90 362.35 10686.1 1000\rdplyr 627.8 673.90 729.7347 697.35 731.95 4953.7 1000\rautoplot(mcb_join) + ggtitle(\u0026quot;Join: Base R vs dplyr\u0026quot;)\ragg_join \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_join), mean)$time\rSo, base R code is on average r max(round(max(agg_join)/agg_join)) times faster than dplyr::left_join. The absolute difference in 10k simulations is r (max(agg_join) - min(agg_join))*1e-5 seconds.\n8. Group and Apply Function 1 # Applying models or transformations to groups of data is a frequent task in statistical workflows. This benchmark showcases the performance of split and lapply in base R versus dplyr::group_map.\nmcb_grp_map \u0026lt;- microbenchmark(\rbase_0 = lapply(split(iris, iris$Species), function(x) lm(Sepal.Length ~ Petal.Length, data = x)),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_map(~ lm(Sepal.Length ~ Petal.Length, data = .x)),\rtimes = 1e3\r)\rmcb_grp_map\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 769.6 841.75 928.2359 872.00 908.85 12631.9 1000\rdplyr 2351.4 2477.05 2678.3166 2556.85 2710.80 5844.0 1000\rautoplot(mcb_grp_map) + ggtitle(\u0026quot;Group Map: Base R vs dplyr\u0026quot;)\ragg_grp_map \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_map), mean)$time\rSo, base R code is on average r max(round(max(agg_grp_map)/agg_grp_map)) times faster than dplyr::group_map. The absolute difference in 10k simulations is r (max(agg_grp_map) - min(agg_grp_map))*1e-5 seconds.\n9. Group and Apply Function 2 # Complex operations on grouped data are common in analytical pipelines. This comparison highlights the performance of base R’s split and lapply with a row-binding step versus dplyr::group_modify.\nmcb_grp_mod \u0026lt;- microbenchmark(\rbase_0 = do.call(rbind, lapply(split(iris, iris$Species), function(x) data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = x))$coefficients))),\rdplyr = iris |\u0026gt;\rgroup_by(Species) |\u0026gt;\rgroup_modify(~ data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = .x))$coefficients)) |\u0026gt;\rungroup(),\rtimes = 1e3\r)\rmcb_grp_mod\rUnit: milliseconds\rexpr min lq mean median uq max neval\rbase_0 1.3757 1.47375 1.660471 1.517 1.5848 40.7728 1000\rdplyr 4.3068 4.47510 4.806882 4.637 4.9067 7.8525 1000\rautoplot(mcb_grp_mod) + ggtitle(\u0026quot;Group Modify: Base R vs dplyr\u0026quot;)\ragg_grp_mod \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_grp_mod), mean)$time\rSo, base R code is on average r max(round(max(agg_grp_mod)/agg_grp_mod)) times faster than dplyr::group_modify. The absolute difference in 10k simulations is r (max(agg_grp_mod) - min(agg_grp_mod))*1e-5 seconds.\n10. Rowwise # mcb_rowwise \u0026lt;- microbenchmark(base_0 = transform(iris, Sepal_sd = apply(cbind(Sepal.Length, Sepal.Width), 1, sd),\rPetal_sd = apply(cbind(Petal.Length, Petal.Width), 1, sd)),\rdplyr = iris |\u0026gt;\rdplyr::rowwise() |\u0026gt;\rdplyr::mutate(Sepal_sd = sd(c(Sepal.Length, Sepal.Width)),\rPetal_sd = sd(c(Petal.Length, Petal.Width))) |\u0026gt;\rdplyr::ungroup(),\rtimes = 1e3)\rautoplot(mcb_rowwise) + ggtitle(\u0026quot;Rowwise: Base R vs dplyr\u0026quot;)\ragg_rowwise \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_rowwise), mean)$time\rSo, base R code is on average r max(round(max(agg_rowwise)/agg_rowwise)) times faster than dplyr::rowwise. The absolute difference in 10k simulations is r (max(agg_rowwise) - min(agg_rowwise))*1e-5 seconds.\n11. Count Rows by Group # Counting the number of rows by group is a simple yet frequent operation. The comparison here emphasizes the efficiency of base R’s table function against dplyr::count.\nmcb_count \u0026lt;- microbenchmark(\rbase_0 = table(iris$Species),\rdplyr = iris |\u0026gt; count(Species),\rtimes = 1e3\r)\rmcb_count\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 15.2 21.5 32.6463 38.00 41.35 230.3 1000\rdplyr 1313.4 1369.7 1479.0262 1410.15 1481.00 3757.0 1000\rautoplot(mcb_count) + ggtitle(\u0026quot;Count: Base R vs dplyr\u0026quot;)\ragg_count \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_count), mean)$time\rSo, base R code is on average r max(round(max(agg_count)/agg_count)) times faster than dplyr::count. The absolute difference in 10k simulations is r (max(agg_count) - min(agg_count))*1e-5 seconds.\n12. Identify Distinct Rows # Identifying unique rows is crucial for deduplication. This benchmark compares base R’s unique function with dplyr::distinct, shedding light on performance differences for this operation.\nmcb_distinct \u0026lt;- microbenchmark(\rbase_0 = unique(rbind(iris, iris)),\rdplyr = distinct(rbind(iris, iris)),\rtimes = 1e3\r)\rmcb_distinct\rUnit: microseconds\rexpr min lq mean median uq max neval\rbase_0 568.8 611.05 668.6889 636.4 664.05 2563.6 1000\rdplyr 237.1 269.20 301.6001 283.2 301.65 2056.8 1000\rautoplot(mcb_distinct) + ggtitle(\u0026quot;Distinct: Base R vs dplyr\u0026quot;)\ragg_distinct \u0026lt;- aggregate(time ~ expr, data = data.frame(mcb_distinct), mean)$time\rSo, base R code is on average r max(round(max(agg_distinct)/agg_distinct)) times slower than dplyr::distinct. The absolute difference in 10k simulations is r (max(agg_distinct) - min(agg_distinct))*1e-5 seconds.\nSummary # Given that these functions are very commonly used, it is fair to assume that these functions are used at least 5 times in a standard simulation code. If that simulation is repeated for 10k times, then the total gain we have by using base R is at least r ((max(agg_filter) - min(agg_filter)) + (max(agg_select) - min(agg_select)) + (max(agg_mutate) - min(agg_mutate)) + (max(agg_summarise) - min(agg_summarise)) + (max(agg_grp) - min(agg_grp)) + (max(agg_sort) - min(agg_sort)) + (max(agg_join) - min(agg_join)) + (max(agg_grp_map) - min(agg_grp_map)) + (max(agg_grp_mod) - min(agg_grp_mod)) + (max(agg_rowwise) - min(agg_rowwise)) + (max(agg_count) - min(agg_count)) - (max(agg_distinct) - min(agg_distinct)))*(5/60)*1e-5 minutes.\nConclusion # This analysis demonstrates\nthat while dplyr offers user-friendly functions and a consistent syntax, base R can often be faster for basic operations, especially with large datasets. The choice between base R and dplyr should consider both readability and computational efficiency, dependent on the scale and complexity of your data tasks.\nFor intensive data simulations, careful function choice can lead to significant performance gains, underscoring the importance of benchmarking and profiling in the R language. "},{"id":1,"href":"/about/","title":"About","section":"Statistically Speaking: An R Journey","content":" About Me # Welcome to my blog!\nI’m an experienced statistician and an avid R enthusiast. Over the years, R has become an integral part of my professional and personal toolkit. Whether it’s performing advanced statistical modeling, developing data visualizations, or exploring machine learning algorithms, I believe R empowers users to push the boundaries of data analysis.\nThis blog is my humble attempt to share what I’ve learned and continue to learn. Here, you’ll find tutorials, tips, and insights about:\nR programming best practices Statistical methods and modeling Data visualization techniques Machine learning in R Performance optimization and comparisons Tools like Shiny, ggplot2, and more I’ll also share examples from real-world scenarios, such as clinical trial data analysis and simulation studies, where R has proven to be invaluable.\nIf you’re passionate about data science, programming, or statistics—or just starting your R journey—you’re in the right place. Let’s grow and learn together, one R script at a time!\nThank you for stopping by, and happy coding!\n"}]