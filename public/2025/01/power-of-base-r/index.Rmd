---
title: "Power of Base R: A Performance Comparison with dplyr"
author: "Swarnendu Chatterjee"
date: "2025-01-17T22:32:22+0530"
draft: false
slug: "power-of-base-R"
categories: ["R", "dplyr", "tidyverse", "base R"]
tags: ["R", "dplyr", "tidyverse", "base R"]
output: hugodown::md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
library(microbenchmark)
library(ggplot2)
library(dplyr)
```

## Introduction

This presentation explores the performance differences between base R and the dplyr package for various data manipulation tasks. 

- While dplyr is renowned for its intuitive syntax and efficiency, 
- base R functions can sometimes outperform it, particularly in large simulations. 

Understanding these differences can aid in making informed decisions when choosing data wrangling techniques.

## The Iris Dataset

- The **iris** dataset is a classic dataset in statistics and machine learning, often used for demonstrating data manipulation and analysis techniques. 
- It contains measurements of flower characteristics for three species of iris: *setosa*, *versicolor*, and *virginica*.

- Key Features:

- **150 observations**: 50 samples for each species.  
- **4 numeric attributes**: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width  
- **1 categorical attribute**: Species: The species of the iris flower (*setosa*, *versicolor*, *virginica*).

## The Iris Dataset (continued)

```{r iris-overview, echo=FALSE}
# Display a few rows of the dataset
iris[c(1:3, 51:53, 101:103), ]
```
```{r echo=FALSE}
# Plot a basic visualization of Sepal.Length vs Sepal.Width colored by Species
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
geom_point() +
labs(
title = "Sepal Dimensions of Iris Flowers",
x = "Sepal Length (cm)",
y = "Sepal Width (cm)"
) +
theme_minimal()
```

## Benchmarking Data Manipulation Tasks

## 1. Filter Rows

Filtering rows is a common operation in data analysis. This section benchmarks the efficiency of base R’s subsetting approaches against the dplyr::filter function. While dplyr provides clean syntax, base R’s performance is more efficient.

```{r filter, message=FALSE, warning=FALSE}
mcb_filter <- microbenchmark(
base_0 = iris[iris$Species == "setosa", ],
base_1 = subset(iris, Species == "setosa"),
dplyr = filter(iris, Species == "setosa"),
times = 1e3
)
mcb_filter
```

---

```{r}
autoplot(mcb_filter) + ggtitle("Filter: Base R vs dplyr")
```

```{r}
agg_filter <- aggregate(time ~ expr, data = data.frame(mcb_filter), mean)$time
```

So, base R code is on average `r max(round(max(agg_filter)/agg_filter))` times **faster** than dplyr::filter. The absolute difference in 10k simulations is `r (max(agg_filter) - min(agg_filter))*1e-5` seconds.

## 2. Select Columns

Selecting specific columns is fundamental to narrowing down datasets. The comparison here shows how base R indexing and the subset function stack up against dplyr::select in terms of speed.

```{r select, message=FALSE, warning=FALSE}
mcb_select <- microbenchmark(
base_0 = iris[, c("Sepal.Length", "Petal.Length")],
base_1 = subset(iris, select = c(Sepal.Length, Petal.Length)),
dplyr = select(iris, Sepal.Length, Petal.Length),
times = 1e3
)
mcb_select
```

---

```{r}
autoplot(mcb_select) + ggtitle("Select: Base R vs dplyr")
```

```{r }
agg_select <- aggregate(time ~ expr, data = data.frame(mcb_select), mean)$time
```

So, base R code is on average `r max(round(max(agg_select)/agg_select))` times **faster** than dplyr::select. The absolute difference in 10k simulations is `r (max(agg_select) - min(agg_select))*1e-5` seconds.

## 3. Add/Modify Columns

Adding or modifying columns is crucial for feature engineering. Base R provides multiple methods for this, which are benchmarked here against dplyr::mutate for their computational efficiency.

```{r mutate, message=FALSE, warning=FALSE}
mcb_mutate <- microbenchmark(
base_0 = {iris$Sepal_LbyW <- iris$Sepal.Length / iris$Sepal.Width},
base_1 = transform(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),
dplyr = mutate(iris, Sepal_LbyW = Sepal.Length / Sepal.Width),
times = 1e3
)
mcb_mutate
```

---

```{r}
autoplot(mcb_mutate) + ggtitle("Mutate: Base R vs dplyr")
```

```{r}
agg_mutate <- aggregate(time ~ expr, data = data.frame(mcb_mutate), mean)$time
```

So, base R code is on average `r max(round(max(agg_mutate)/agg_mutate))` times **faster** than dplyr::mutate. The absolute difference in 10k simulations is `r (max(agg_mutate) - min(agg_mutate))*1e-5` seconds.

## 4. Summarise Data

Data summarization helps derive aggregate metrics. This section compares colMeans from base R with dplyr::summarise_if, illustrating their relative performance in summarizing numeric columns.

```{r summarise, message=FALSE, warning=FALSE}
mcb_summarise <- microbenchmark(
base_0 = with(iris, data.frame(Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),
Petal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width))),
dplyr = summarise(iris, Sepal_L_mean = mean(Sepal.Length), Sepal_W_sd = sd(Sepal.Width),
Petal_L_mean = mean(Petal.Length), Petal_W_sd = sd(Petal.Width)),
times = 1e3
)
mcb_summarise
```

---

```{r}
autoplot(mcb_summarise) + ggtitle("Summarise: Base R vs dplyr")
```

```{r }
agg_summarise <- aggregate(time ~ expr, data = data.frame(mcb_summarise), mean)$time
```

So, base R code is on average `r max(round(max(agg_summarise)/agg_summarise))` times **faster** than dplyr::summarise. The absolute difference in 10k simulations is `r (max(agg_summarise) - min(agg_summarise))*1e-5` seconds.

## 5. Grouped Summary

Aggregating data by groups is often required for advanced analytics. Base R’s aggregate function is tested here against dplyr's group_by and summarise_all functions to highlight performance differences.

```{r grp_summ, message=FALSE, warning=FALSE}
mcb_grp <- microbenchmark(
base_0 = aggregate(. ~ Species, data = iris, FUN = mean, na.rm = TRUE),
dplyr = iris |>
group_by(Species) |>
summarise_all(mean, na.rm = TRUE) |>
ungroup(),
times = 1e3
)
mcb_grp
```

---

```{r}
autoplot(mcb_grp) + ggtitle("Grouped Summarise: Base R vs dplyr")
```

```{r }
agg_grp <- aggregate(time ~ expr, data = data.frame(mcb_grp), mean)$time
```

So, base R code is on average `r max(round(max(agg_grp)/agg_grp))` times **faster** than dplyr::grp. The absolute difference in 10k simulations is `r (max(agg_grp) - min(agg_grp))*1e-5` seconds.

## 6. Sort Data

Sorting is essential for ordering data before visualization or further analysis. The benchmarks here compare the classic base R order approach to the elegant dplyr::arrange.

```{r sort, message=FALSE, warning=FALSE}
mcb_sort <- microbenchmark(
base_0 = iris[with(iris, order(Sepal.Length, Petal.Length)), ],
dplyr = arrange(iris, Sepal.Length, Petal.Length),
times = 1e3
)
mcb_sort
```

---

```{r}
autoplot(mcb_sort) + ggtitle("Sort: Base R vs dplyr")
```

```{r }
agg_sort <- aggregate(time ~ expr, data = data.frame(mcb_sort), mean)$time
```

So, base R code is on average `r max(round(max(agg_sort)/agg_sort))` times **faster** than dplyr::arrange. The absolute difference in 10k simulations is `r (max(agg_sort) - min(agg_sort))*1e-5` seconds.

## 7. Join Data

Data joins are indispensable when working with relational datasets. This section demonstrates the efficiency of base R’s merge function compared to dplyr::left_join.

```{r join, message=FALSE, warning=FALSE}
iris$id <- sample.int(nrow(iris), replace = FALSE)
iris2 <- iris[sample(nrow(iris)),]
mcb_join <- microbenchmark(
base_0 = merge(iris, iris2, by = "id", all.x = TRUE),
dplyr = left_join(iris, iris2, by = "id"),
times = 1e3
)
mcb_join
```

---

```{r}
autoplot(mcb_join) + ggtitle("Join: Base R vs dplyr")
```

```{r}
agg_join <- aggregate(time ~ expr, data = data.frame(mcb_join), mean)$time
```

So, base R code is on average `r max(round(max(agg_join)/agg_join))` times **faster** than dplyr::left_join. The absolute difference in 10k simulations is `r (max(agg_join) - min(agg_join))*1e-5` seconds.

## 8. Group and Apply Function 1

Applying models or transformations to groups of data is a frequent task in statistical workflows. This benchmark showcases the performance of split and lapply in base R versus dplyr::group_map.

```{r grp_map, message=FALSE, warning=FALSE}
mcb_grp_map <- microbenchmark(
base_0 = lapply(split(iris, iris$Species), function(x) lm(Sepal.Length ~ Petal.Length, data = x)),
dplyr = iris |>
group_by(Species) |>
group_map(~ lm(Sepal.Length ~ Petal.Length, data = .x)),
times = 1e3
)
mcb_grp_map
```

---

```{r}
autoplot(mcb_grp_map) + ggtitle("Group Map: Base R vs dplyr")
```

```{r}
agg_grp_map <- aggregate(time ~ expr, data = data.frame(mcb_grp_map), mean)$time
```

So, base R code is on average `r max(round(max(agg_grp_map)/agg_grp_map))` times **faster** than dplyr::group_map. The absolute difference in 10k simulations is `r (max(agg_grp_map) - min(agg_grp_map))*1e-5` seconds.

## 9. Group and Apply Function 2

Complex operations on grouped data are common in analytical pipelines. This comparison highlights the performance of base R’s split and lapply with a row-binding step versus dplyr::group_modify.

```{r grp_mod, message=FALSE, warning=FALSE}
mcb_grp_mod <- microbenchmark(
base_0 = do.call(rbind, lapply(split(iris, iris$Species), function(x) 
data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = x))$coefficients))),
dplyr = iris |>
group_by(Species) |>
group_modify(~ data.frame(summary(lm(Sepal.Length ~ Petal.Length, data = .x))$coefficients)) |>
ungroup(),
times = 1e3
)
mcb_grp_mod
```

---

```{r}
autoplot(mcb_grp_mod) + ggtitle("Group Modify: Base R vs dplyr")
```

```{r}
agg_grp_mod <- aggregate(time ~ expr, data = data.frame(mcb_grp_mod), mean)$time
```

So, base R code is on average `r max(round(max(agg_grp_mod)/agg_grp_mod))` times **faster** than dplyr::group_modify. The absolute difference in 10k simulations is `r (max(agg_grp_mod) - min(agg_grp_mod))*1e-5` seconds.

## 10. Rowwise

```{r rowwise, message=FALSE, warning=FALSE}
mcb_rowwise <- microbenchmark(base_0 = transform(iris, 
Sepal_sd = apply(cbind(Sepal.Length, Sepal.Width), 1, sd),
Petal_sd = apply(cbind(Petal.Length, Petal.Width), 1, sd)),
dplyr = iris |>
dplyr::rowwise() |>
dplyr::mutate(Sepal_sd = sd(c(Sepal.Length, Sepal.Width)),
Petal_sd = sd(c(Petal.Length, Petal.Width))) |>
dplyr::ungroup(),
times = 1e3)
```

---

```{r}
autoplot(mcb_rowwise) + ggtitle("Rowwise: Base R vs dplyr")
```

```{r}
agg_rowwise <- aggregate(time ~ expr, data = data.frame(mcb_rowwise), mean)$time
```

So, base R code is on average `r max(round(max(agg_rowwise)/agg_rowwise))` times **faster** than dplyr::rowwise. The absolute difference in 10k simulations is `r (max(agg_rowwise) - min(agg_rowwise))*1e-5` seconds.

## 11. Count Rows by Group

Counting the number of rows by group is a simple yet frequent operation. The comparison here emphasizes the efficiency of base R’s table function against dplyr::count.

```{r count, message=FALSE, warning=FALSE}
mcb_count <- microbenchmark(
base_0 = table(iris$Species),
dplyr = iris |> count(Species),
times = 1e3
)
mcb_count
```

---

```{r}
autoplot(mcb_count) + ggtitle("Count: Base R vs dplyr")
```

```{r}
agg_count <- aggregate(time ~ expr, data = data.frame(mcb_count), mean)$time
```

So, base R code is on average `r max(round(max(agg_count)/agg_count))` times **faster** than dplyr::count. The absolute difference in 10k simulations is `r (max(agg_count) - min(agg_count))*1e-5` seconds.

## 12. Identify Distinct Rows

Identifying unique rows is crucial for deduplication. This benchmark compares base R’s unique function with dplyr::distinct, shedding light on performance differences for this operation.

```{r distinct, message=FALSE, warning=FALSE}
mcb_distinct <- microbenchmark(
base_0 = unique(rbind(iris, iris)),
dplyr = distinct(rbind(iris, iris)),
times = 1e3
)
mcb_distinct
```

---

```{r}
autoplot(mcb_distinct) + ggtitle("Distinct: Base R vs dplyr")
```

```{r}
agg_distinct <- aggregate(time ~ expr, data = data.frame(mcb_distinct), mean)$time
```

So, base R code is on average `r max(round(max(agg_distinct)/agg_distinct))` times **slower** than dplyr::distinct. The absolute difference in 10k simulations is `r (max(agg_distinct) - min(agg_distinct))*1e-5` seconds. 

## Summary

Given that these functions are very commonly used, it is fair to assume that these functions are used at least 5 times in a standard simulation code. If that simulation is repeated for 10k times, then the total gain we have by using base R is at least `r ((max(agg_filter) - min(agg_filter)) + (max(agg_select) - min(agg_select)) + (max(agg_mutate) - min(agg_mutate)) + (max(agg_summarise) - min(agg_summarise)) + (max(agg_grp) - min(agg_grp)) + (max(agg_sort) - min(agg_sort)) + (max(agg_join) - min(agg_join)) + (max(agg_grp_map) - min(agg_grp_map)) + (max(agg_grp_mod) - min(agg_grp_mod))  + (max(agg_rowwise) - min(agg_rowwise)) + (max(agg_count) - min(agg_count)) - (max(agg_distinct) - min(agg_distinct)))*(5/60)*1e-5` minutes.

## Conclusion

This analysis demonstrates

- that while dplyr offers user-friendly functions and a consistent syntax, 
- base R can often be **faster** for basic operations, especially with large datasets. 

The choice between base R and dplyr should consider both readability and computational efficiency, dependent on the scale and complexity of your data tasks.

- For intensive data simulations, careful function choice can lead to significant performance gains, 
- underscoring the importance of benchmarking and profiling in the R language.