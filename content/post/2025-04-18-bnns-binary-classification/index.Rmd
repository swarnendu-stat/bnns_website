---
title: "BNNs in Binary Classification: A Bayesian Take on Clinical Prediction"
author: "Swarnendu Chatterjee"
date: "2025-04-18T22:21:00+0530"
slug: bnns-binary-classification
categories: ["Biostatistics", "Machine Learning", "Bayesian Methods"]
tags: ["R", "Bayesian Neural Networks", "Clinical Trials"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
```

## ğŸ” Can Bayesian Neural Nets Classify Better Than Your Favorite Models?

We often talk about uncertainty in models, but how often do we actually quantify it?\
Enter `bnns`, an R package that lets you fit **Bayesian Neural Networks** (BNNs) with the elegance of `glm()` and the insight of a statistician.

In this post, weâ€™ll walk through a real-world binary classification example: predicting respiratory status using clinical trial data. Then, weâ€™ll pit our BNN against the classic logistic regression and the ever-popular random forest.

Let the models battle it outâ€”Bayesian style.

### ğŸ§ª The Data: Clean, Clinical, and Real

We use the `respiratory` dataset from the `HSAUR3` package, focusing on patients from **month 4**. The binary outcome is whether the patient's status is "good". Simple, interpretable, and grounded in realityâ€”just how statisticians like it.

```{r data}
library(bnns)
library(HSAUR3)
library(rsample)
library(ggplot2)
library(randomForest)

set.seed(123)
trial_data <- HSAUR3::respiratory |>
  subset(month == 4, select = - c(subject, month)) |>
  transform(status = ifelse(status == "good", 1, 0))

trial_data_split <- initial_split(trial_data, strata = status, prop = 0.8)

trial_data_train <- training(trial_data_split)
trial_data_test <- testing(trial_data_split)
```

We stratify by outcome during data splitting to ensure balance. BNNs appreciate thoughtful input.

### ğŸ§  Building the BNN: No Black Box Here

```{r bnns}
bnn_model <- bnns(
  status ~ .,
  data = trial_data_train,
  L = 3,
  nodes = c(64, 4, 4),
  act_fn = rep(4, 3),
  out_act_fn = 2,
  iter = 1e3,
  warmup = 2e2,
  chains = 2,
  prior_weights = list(dist = "normal", params = list(mean = 0, sd = 1)),
  prior_bias = list(dist = "normal", params = list(mean = 0, sd = 10)),
  prior_sigma = list(dist = "half_normal", params = list(mean = 0, sd = 1))
)
```

With just a few lines, weâ€™ve defined a three-layer BNN with fully Bayesian uncertainty on weights, biases, and the residual scale. And no, you donâ€™t need a PhD in Stan to do thisâ€”`bnns` handles the machinery under the hood.

### ğŸ”® Predictions with a Side of Uncertainty

Unlike point estimates from GLMs or RFs, BNNs give us full **posterior distributions** for predictions. We grab medians and 95% credible intervalsâ€”because whatâ€™s a prediction without a little humility?

```{r pred}
pred <- predict(bnn_model, newdata = trial_data_test)
pred_y <- apply(pred, MARGIN = 1, median, na.rm = TRUE)
pred_quantiles <- apply(pred, MARGIN = 1, function(x)quantile(x, probs = c(0.025, 0.975), na.rm = TRUE), simplify = FALSE) |>
  do.call(args = _, what = rbind)
measure_bin(trial_data_test$status, pred)
```

We then evaluate performance using `measure_bin()`, our in-house accuracy function.

### ğŸ“Š The Plot Thickens

We visualize predictions with credible intervals to see where the model hesitatesâ€”and thatâ€™s the beauty of Bayesian models. They donâ€™t bluff.

```{r plot}
plot_data <- data.frame(
  Actual = trial_data_test$status,
  Predicted = pred_y,
  Lower = pred_quantiles[,1],
  Upper = pred_quantiles[,2]
) |>
  transform(width = Upper - Lower) |>
  dplyr::arrange(width)

head(plot_data, 5)
```

BNN predictions with 95% credible intervals for test observations. While most intervals are reasonably narrow, wider intervals reflect situations where the model senses ambiguity â€” not a bug, but a valuable signal.

Letâ€™s look at a sample of confident predictions with interpretable intervals.

```{r sample_plot}
ggplot(head(plot_data, 5), aes(x = Actual, y = Predicted)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.05, color = "steelblue") +
  labs(title = "BNN Predictions for Patient Status",
    subtitle = "Error bars show 95% credible intervals",
    x = "Test Set Patient Status",
    y = "Predicted Patient Status"
  ) +
  theme_minimal()
```

Unlike black-box predictions, BNNs offer a quantified view of uncertainty â€” a key advantage in high-stakes decision-making.

### ğŸ¥Š The Showdown: GLM vs RF vs BNN

```{r compare}
glm_model <- glm(status ~ .,
                 data = trial_data_train)
glm_pred <- predict(glm_model, trial_data_test)
measure_bin(trial_data_test$status, glm_pred)

rf_model <- randomForest(factor(status) ~ .,
                 data = trial_data_train)
rf_pred <- predict(rf_model, trial_data_test, type = "prob")[,2]
measure_bin(trial_data_test$status, rf_pred)
```

We compare three models:

-   **GLM:** The old reliable.
-   **Random Forest:** The default go-to.
-   **BNN:** The fresh Bayesian face in town.

BNNs didnâ€™t just hold their ownâ€”they brought nuance. Theyâ€™re not just saying â€œyesâ€ or â€œnoâ€, theyâ€™re saying â€œprobably yes, and hereâ€™s how sure I am.â€

### ğŸ’¡ Why Use `bnns`?

-   **Uncertainty built-in**: Every prediction comes with a measure of confidence.
-   **Flexible priors**: Tailor regularization like a true Bayesian.
-   **Transparent**: Uses `rstan` under the hood, no magic involved.
-   **Light syntax**: As easy as `glm()`, but smarter.

### ğŸš€ Final Thoughts

In clinical settings, decisions arenâ€™t just about accuracyâ€”theyâ€™re about **confidence**. Thatâ€™s where BNNs shine. And with `bnns`, you donâ€™t have to trade usability for rigor.

Next time you're modeling binary outcomes, ask yourself:\
\> *Do I just want a prediction, or do I want to understand the uncertainty behind it?*
